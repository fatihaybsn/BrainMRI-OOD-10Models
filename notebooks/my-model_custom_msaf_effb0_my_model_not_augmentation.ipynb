{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ad5794",
   "metadata": {},
   "source": [
    "# üß† Brain MRI ƒ∞kili Sƒ±nƒ±flandƒ±rma ‚Äî Modeli Tek Tek Se√ßerek Eƒüitim\n",
    "\n",
    "\n",
    "Bu defter, **sadece ikili sƒ±nƒ±flandƒ±rma** (labels: **`tumor`** ve **`no_tumor`**) i√ßin d√ºzenlendi.\n",
    "A≈üaƒüƒ±daki h√ºcreler ile **modeli tek tek kendiniz se√ßip** eƒüitebilir; her eƒüitim i√ßin **ayrƒ± ayrƒ±**:\n",
    "- Accuracy grafiƒüi,\n",
    "- Loss grafiƒüi,\n",
    "- **Hata matrisi (Confusion Matrix)**,\n",
    "- **ROC eƒürisi (AUC ile)**,\n",
    "- ve **tablo halinde Sensitivity (Recall), Precision, F1, Cohen‚Äôs Kappa**\n",
    "\n",
    "olu≈üturup **PNG olarak kaydedebilirsiniz**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441cfc8",
   "metadata": {},
   "source": [
    "## üì¶ Kurulumlar ve K√ºt√ºphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e670597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install -q timm scikit-learn torchmetrics\n",
    "\n",
    "import os, math, time, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm  # √ßok sayƒ±da SOTA mimari\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reprod√ºksiyon\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# DataLoader i≈ü√ßileri i√ßin deterministik davranƒ±≈ü\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# (Opsiyonel) PyTorch 2.x‚Äôte matmul hesaplarƒ±nƒ± ‚Äúhigh‚Äù hassasiyete √ßekerek olasƒ± hƒ±z optimizasyonu deniyor; desteklenmezse sessizce ge√ßiyor.\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 1) # DataLoader i√ßin en fazla 4 olmak √ºzere CPU √ßekirdek sayƒ±sƒ± kadar i≈ü√ßi belirliyor.\n",
    "BATCH_SIZE = 32\n",
    "VAL_RATIO = 0.1   # verinin %10‚Äôu validasyon i√ßin ayrƒ±lacak.\n",
    "MAX_EPOCHS_DEFAULT = 50 \n",
    "AUG_STRENGTH = 0.0  # 0.0 => aug kapalƒ±, 0.3 => agresif (train tarafƒ±nda)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb666295",
   "metadata": {},
   "source": [
    "## üß≠ Yollar ve Temel Ayarlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d029f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß Dƒ∞Zƒ∞N YAPISI\n",
    "# DATA_ROOT:\n",
    "#   ‚îú‚îÄ‚îÄ Training\n",
    "#   ‚îÇ    ‚îú‚îÄ‚îÄ tumor\n",
    "#   ‚îÇ    ‚îî‚îÄ‚îÄ no_tumor\n",
    "#   ‚îî‚îÄ‚îÄ Testing\n",
    "#        ‚îú‚îÄ‚îÄ tumor\n",
    "#        ‚îî‚îÄ‚îÄ no_tumor\n",
    "\n",
    "DATA_ROOT = \"Dataset\"  # Veri seti k√∂k dizin.\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"Training\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"Testing\")\n",
    "\n",
    "# Etiket sƒ±rasƒ± sabitliyorum: 0 = no_tumor, 1 = tumor\n",
    "CLASS_NAMES = [\"no_tumor\", \"tumor\"]\n",
    "POS_LABEL = 1  # ROC/metrics i√ßin pozitif sƒ±nƒ±f (tumor)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True) # √áƒ±ktƒ±lar i√ßin results klas√∂r√º yoksa olu≈üturuluyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadcc60",
   "metadata": {},
   "source": [
    "## üß© Model Profilleri (Input Size) ve Tekli Se√ßim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709ae705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: custom_msaf_effb0 Input: 256\n"
     ]
    }
   ],
   "source": [
    "# Giri≈ü boyutu √∂nerileri\n",
    "MODEL_PROFILES = {\n",
    "    \"resnet34\": {\"input_size\": 224},\n",
    "    \"resnet50\": {\"input_size\": 224},\n",
    "    \"densenet121\": {\"input_size\": 224},\n",
    "    \"inception_v3\": {\"input_size\": 299},\n",
    "    \"efficientnet_b0\": {\"input_size\": 224},\n",
    "    \"mobilenetv2_100\": {\"input_size\": 224},\n",
    "    \"convnext_tiny\": {\"input_size\": 224},\n",
    "\n",
    "    # ‚úÖ HYBRID\n",
    "    \"hybrid_dn121_effb0\": {\"input_size\": 256},\n",
    "\n",
    "    # ‚úÖ √ñZG√úN MODEL (Custom)\n",
    "    \"custom_msaf_effb0\": {\"input_size\": 256},\n",
    "}\n",
    "\n",
    "ALL_MODELS = list(MODEL_PROFILES.keys())\n",
    "\n",
    "# ‚úÖ Senin kararƒ±n: dataset karƒ±≈üƒ±k olduƒüu i√ßin her ≈üeyi 256'ya sabitle\n",
    "OVERRIDE_INPUT_SIZE = 256\n",
    "\n",
    "# Buradan TEK bir modeli se√ßin\n",
    "SELECT_ONE_MODEL = \"custom_msaf_effb0\"  # <- √∂zg√ºn modeli se√ßtik\n",
    "TARGET_MODEL = SELECT_ONE_MODEL\n",
    "\n",
    "INPUT_SIZE = OVERRIDE_INPUT_SIZE if OVERRIDE_INPUT_SIZE is not None else MODEL_PROFILES[SELECT_ONE_MODEL][\"input_size\"]\n",
    "print(\"Selected:\", SELECT_ONE_MODEL, \"Input:\", INPUT_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ecac0",
   "metadata": {},
   "source": [
    "## üñºÔ∏è D√∂n√º≈ü√ºmler (Pad + Resize + Normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2710f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "\n",
    "# Model bazlƒ± mean/std cache\n",
    "_MEAN_STD_CACHE = {}\n",
    "\n",
    "# ‚úÖ Hibrit i√ßin mean/std hangi backbone'dan alƒ±nacak?\n",
    "_HYBRID_MEANSTD_SOURCE = {\n",
    "    \"hybrid_dn121_effb0\": \"densenet121\",  # ikisi de ImageNet normalize kullandƒ±ƒüƒ± i√ßin fark etmez\n",
    "    \"custom_msaf_effb0\": \"efficientnet_b0\",  # EfficientNet-B0 backbone normalize\n",
    "}\n",
    "\n",
    "def get_mean_std_from_timm(model_name: str):\n",
    "    \"\"\"\n",
    "    timm modelinin default_cfg/pretrained_cfg i√ßinden mean/std √ßeker.\n",
    "    Hibrit model adƒ±nƒ± desteklemek i√ßin backbone'a map edilir.\n",
    "    \"\"\"\n",
    "    name = _HYBRID_MEANSTD_SOURCE.get(model_name, model_name)\n",
    "\n",
    "    if name in _MEAN_STD_CACHE:\n",
    "        return _MEAN_STD_CACHE[name]\n",
    "\n",
    "    m = timm.create_model(name, pretrained=False, num_classes=2)\n",
    "    cfg = resolve_data_config({}, model=m)\n",
    "\n",
    "    mean, std = cfg[\"mean\"], cfg[\"std\"]\n",
    "    _MEAN_STD_CACHE[name] = (mean, std)\n",
    "    del m\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            return img\n",
    "        size = max(w, h)\n",
    "        pad_left = (size - w) // 2\n",
    "        pad_top = (size - h) // 2\n",
    "        pad_right = size - w - pad_left\n",
    "        pad_bottom = size - h - pad_top\n",
    "        return transforms.functional.pad(\n",
    "            img, (pad_left, pad_top, pad_right, pad_bottom), fill=0\n",
    "        )\n",
    "\n",
    "\n",
    "def build_transforms(model_name: str, input_size: int, aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Transform √ºretimi.\n",
    "\n",
    "    aug_strength:\n",
    "      - 0.0  => augmentasyon kapalƒ± (train = eval)\n",
    "      - 0.0 < => train'de augmentasyon a√ßƒ±k (0.3 = agresif)\n",
    "    Not: aug_strength 0.3 √ºst√º verilirse 0.3'e kƒ±rpƒ±lƒ±r.\n",
    "    \"\"\"\n",
    "    mean, std = get_mean_std_from_timm(model_name)\n",
    "\n",
    "    base = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    # Aug kapalƒ±ysa train = eval (tam istediƒüin davranƒ±≈ü)\n",
    "    try:\n",
    "        a = float(aug_strength)\n",
    "    except Exception:\n",
    "        a = 0.0\n",
    "    a = max(0.0, min(0.3, a))\n",
    "    if a <= 0.0:\n",
    "        return base, base\n",
    "\n",
    "    # 0..1 √∂l√ßeƒüe ta≈üƒ± (0.3 => 1.0 yoƒüunluk)\n",
    "    s = a / 0.3\n",
    "\n",
    "    # PIL seviyesinde (ToTensor'dan √∂nce) augmentasyonlar\n",
    "    pil_aug = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.50 * s),\n",
    "        transforms.RandomVerticalFlip(p=0.15 * s),\n",
    "        transforms.RandomRotation(degrees=15.0 * s),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0.0,\n",
    "            translate=(0.08 * s, 0.08 * s),\n",
    "            scale=(1.0 - 0.10 * s, 1.0 + 0.10 * s),\n",
    "        ),\n",
    "        transforms.RandomPerspective(distortion_scale=0.20 * s, p=0.25 * s),\n",
    "        transforms.RandomApply(\n",
    "            [transforms.ColorJitter(\n",
    "                brightness=0.40 * s,\n",
    "                contrast=0.40 * s,\n",
    "                saturation=0.25 * s,\n",
    "                hue=0.05 * s,\n",
    "            )],\n",
    "            p=0.70 * s\n",
    "        ),\n",
    "        transforms.RandomApply(\n",
    "            [transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5 * s + 0.1))],\n",
    "            p=0.20 * s\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        pil_aug,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "        # Tensor seviyesinde augmentasyon (masking benzeri regularization)\n",
    "        transforms.RandomErasing(\n",
    "            p=0.25 * s,\n",
    "            scale=(0.02, 0.10),\n",
    "            ratio=(0.3, 3.3),\n",
    "            value=\"random\"\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    eval_tfms = base\n",
    "    return train_tfms, eval_tfms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59819689",
   "metadata": {},
   "source": [
    "## üì• Dataset & DataLoader'lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9673c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def make_dataloaders(model_name: str, input_size: int, batch_size: int = BATCH_SIZE, val_ratio: float = VAL_RATIO, aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Train/Val/test DataLoader kurulumunu doƒüru transform'larla yapar.\n",
    "    - Train: train_tfms\n",
    "    - Val/Test: eval_tfms\n",
    "    Ayrƒ±ca train subset'i geri d√∂nd√ºr√ºr (sƒ±nƒ±f aƒüƒ±rlƒ±klarƒ± i√ßin).\n",
    "    \"\"\"\n",
    "    train_tfms, eval_tfms = build_transforms(model_name, input_size, aug_strength=aug_strength)\n",
    "\n",
    "    # 1) Stratified split i√ßin base dataset + etiketler\n",
    "    base_ds = datasets.ImageFolder(TRAIN_DIR)  # transform YOK\n",
    "    y = np.array(base_ds.targets)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)\n",
    "    train_indices, val_indices = next(sss.split(np.zeros(len(y)), y))\n",
    "\n",
    "    # 2) Ayrƒ± g√∂r√ºn√ºmler: train vs val/test i√ßin farklƒ± transform\n",
    "    train_view = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "    val_view   = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
    "    test_ds    = datasets.ImageFolder(TEST_DIR,  transform=eval_tfms)\n",
    "\n",
    "    # 3) ƒ∞ndeksleri Subset'lere uygula\n",
    "    train_ds = Subset(train_view, train_indices.tolist())\n",
    "    val_ds   = Subset(val_view,   val_indices.tolist())\n",
    "\n",
    "    # 4) (Opsiyonel) Sƒ±nƒ±f sƒ±rasƒ± kontrol√º\n",
    "    expected = [\"no_tumor\", \"tumor\"]\n",
    "    assert train_view.classes == expected, f\"Sƒ±nƒ±f sƒ±rasƒ± {train_view.classes} beklenen {expected} deƒüil!\"\n",
    "\n",
    "    # 5) DataLoader'lar (deterministik workers)\n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    pin = (DEVICE.type == \"cuda\")\n",
    "    common = dict(num_workers=NUM_WORKERS, pin_memory=pin, worker_init_fn=seed_worker, persistent_workers=bool(NUM_WORKERS))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  generator=gen, **common)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076ecb9",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model & Kayƒ±p Fonksiyonu & Optimizasyon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ √ñzg√ºn Model: CustomMSAF-EffB0 (Multi‚ÄëScale Attention Fusion)\n",
    "\n",
    "Bu notebook‚Äôta **timm EfficientNet‚ÄëB0 backbone** kullanƒ±p, backbone‚Äôun **3 farklƒ± √∂l√ßekten** √ºrettiƒüi feature map‚Äôleri alƒ±p:\n",
    "\n",
    "- Her √∂l√ßekte **1√ó1 projeksiyon (kanal e≈üitleme)** + **SE (channel attention)** uyguluyorum\n",
    "- Her √∂l√ßekte **Global Average Pooling** ile vekt√∂r √ßƒ±karƒ±yorum\n",
    "- √ñl√ßekler arasƒ± **√∂ƒürenilebilir attention aƒüƒ±rlƒ±klarƒ±** ile ‚Äúhangi √∂l√ßek daha √∂nemli?‚Äù sorusunu modelin √∂ƒürenmesini saƒülƒ±yorum\n",
    "- Sonunda **[fused_vector + t√ºm √∂l√ßek vekt√∂rleri]** birle≈üimi ile MLP head √ºzerinden **2 sƒ±nƒ±f logits** √ºretiyorum\n",
    "\n",
    "Bu yapƒ±, tek bir backbone ile ‚Äúhibrit benzeri‚Äù √ßok‚Äë√∂l√ßekli f√ºzyon saƒülar ve mevcut eƒüitim/√ßƒ±ktƒ± kayƒ±t mantƒ±ƒüƒ±nƒ± bozmaz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7da69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# üèóÔ∏è Model Tanƒ±mlarƒ±\n",
    "# =========================\n",
    "\n",
    "class HybridDN121_EffB0(nn.Module):\n",
    "    \"\"\"\n",
    "    DenseNet121 + EfficientNetB0 hibrit:\n",
    "    - ƒ∞ki backbone'dan pooled feature √ßƒ±karƒ±r (num_classes=0, global_pool='avg')\n",
    "    - Concatenate + k√º√ß√ºk bir MLP head ile 2 sƒ±nƒ±f logits √ºretir\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, head_dim=256, dropout=0.2, freeze_backbones=False):\n",
    "        super().__init__()\n",
    "        self.bb1 = timm.create_model(\"densenet121\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.bb2 = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        if freeze_backbones:\n",
    "            for p in self.bb1.parameters(): \n",
    "                p.requires_grad = False\n",
    "            for p in self.bb2.parameters(): \n",
    "                p.requires_grad = False\n",
    "\n",
    "        dim1 = getattr(self.bb1, \"num_features\")\n",
    "        dim2 = getattr(self.bb2, \"num_features\")\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(dim1 + dim2, head_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_dim, num_classes)   # ‚úÖ logits (softmax yok)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.bb1(x)\n",
    "        f2 = self.bb2(x)\n",
    "        feats = torch.cat([f1, f2], dim=1)\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation (Channel Attention)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(1, channels // reduction)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, hidden, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class Conv1x1BNAct(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class CustomMSAF_EffB0(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ √ñzg√ºn Model (Custom):\n",
    "    EfficientNet-B0 backbone + Multi-Scale Attention Fusion (MSAF)\n",
    "\n",
    "    - EfficientNet-B0 (features_only) ile 3 farklƒ± √∂l√ßekten feature map alƒ±r\n",
    "    - Her √∂l√ßeƒüe 1x1 projeksiyon + SE uygular\n",
    "    - Her √∂l√ßekten GAP ile vekt√∂r √ßƒ±karƒ±r\n",
    "    - √ñl√ßekler arasƒ± attention ile aƒüƒ±rlƒ±klƒ± f√ºzyon yapar\n",
    "    - fused_vector + concat(all_scales) -> MLP head -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 2, embed_dim: int = 256, head_dim: int = 256,\n",
    "                 dropout: float = 0.35, freeze_backbone: bool = False, out_indices=(2, 3, 4)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            \"efficientnet_b0\",\n",
    "            pretrained=True,\n",
    "            features_only=True,\n",
    "            out_indices=out_indices\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        chs = self.backbone.feature_info.channels()\n",
    "        self.proj = nn.ModuleList([Conv1x1BNAct(c, embed_dim) for c in chs])\n",
    "        self.se   = nn.ModuleList([SEBlock(embed_dim, reduction=16) for _ in chs])\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # √ñl√ßek attention: her √∂l√ßek vekt√∂r√ºnden 1 skor √ºret (softmax ile normalize edilir)\n",
    "        att_hidden = max(8, embed_dim // 4)\n",
    "        self.scale_attn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, att_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.25),\n",
    "            nn.Linear(att_hidden, 1)\n",
    "        )\n",
    "\n",
    "        in_dim = embed_dim * (len(chs) + 1)  # fused + concat(all)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_dim, head_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)  # list[tensor] (B,C,H,W)\n",
    "\n",
    "        vecs = []\n",
    "        for f, proj, se in zip(feats, self.proj, self.se):\n",
    "            f = se(proj(f))\n",
    "            v = self.pool(f).flatten(1)  # (B, embed_dim)\n",
    "            vecs.append(v)\n",
    "\n",
    "        ms = torch.stack(vecs, dim=1)  # (B, S, E)\n",
    "        att_logits = self.scale_attn(ms).squeeze(-1)  # (B, S)\n",
    "        w = torch.softmax(att_logits, dim=1)          # (B, S)\n",
    "        fused = (ms * w.unsqueeze(-1)).sum(dim=1)     # (B, E)\n",
    "\n",
    "        concat_all = ms.flatten(1)                    # (B, S*E)\n",
    "        z = torch.cat([fused, concat_all], dim=1)     # (B, E + S*E)\n",
    "        return self.head(z)\n",
    "\n",
    "\n",
    "def build_model(model_name: str, num_classes: int = 2):\n",
    "    # ‚úÖ Hibrit\n",
    "    if model_name == \"hybrid_dn121_effb0\":\n",
    "        # freeze_backbones=True dersen overfitting azalƒ±r ama tavan performansƒ± d√º≈üebilir\n",
    "        return HybridDN121_EffB0(num_classes=num_classes, head_dim=256, dropout=0.2, freeze_backbones=False)\n",
    "\n",
    "    # ‚úÖ √ñzg√ºn model\n",
    "    if model_name == \"custom_msaf_effb0\":\n",
    "        return CustomMSAF_EffB0(num_classes=num_classes, embed_dim=256, head_dim=256, dropout=0.35, freeze_backbone=False)\n",
    "\n",
    "    # Normal timm modeller\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    ImageFolder ya da Subset(ImageFolder) kabul eder.\n",
    "    \"\"\"\n",
    "    # Subset ise hedef etiketleri indekslerden topla\n",
    "    if isinstance(dataset, Subset):\n",
    "        base = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "        targets = getattr(base, \"targets\", None)\n",
    "        if targets is None:\n",
    "            raise ValueError(\"Temel dataset'te 'targets' bulunamadƒ±.\")\n",
    "        labels = [targets[i] for i in indices]\n",
    "    else:\n",
    "        labels = list(getattr(dataset, \"targets\", []))\n",
    "\n",
    "    n0 = sum(1 for y in labels if y == 0)\n",
    "    n1 = sum(1 for y in labels if y == 1)\n",
    "    total = max(1, n0 + n1)\n",
    "    # Basit ters frekans aƒüƒ±rlƒ±klandƒ±rmasƒ±\n",
    "    w0 = total / (2.0 * max(1, n0))\n",
    "    w1 = total / (2.0 * max(1, n1))\n",
    "    return torch.tensor([w0, w1], dtype=torch.float)\n",
    "\n",
    "def make_optimizer(model, lr=3e-4, weight_decay=1e-4):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600cff1",
   "metadata": {},
   "source": [
    "## üîÅ Eƒüitim & Doƒürulama D√∂ng√ºs√º (History ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7926b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class History:\n",
    "    train_loss: list\n",
    "    val_loss: list\n",
    "    train_acc: list\n",
    "    val_acc: list\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def run_one_epoch(model, loader, criterion, optimizer=None, scaler: GradScaler = None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler is not None and DEVICE.type == \"cuda\":\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "def train_model_one_run(model_name: str,\n",
    "                        max_epochs: int = MAX_EPOCHS_DEFAULT,\n",
    "                        lr: float = 3e-4,\n",
    "                        weight_decay: float = 1e-4,\n",
    "                        batch_size: int = BATCH_SIZE,\n",
    "                        aug_strength: float = 0.0):\n",
    "    input_size = OVERRIDE_INPUT_SIZE if OVERRIDE_INPUT_SIZE is not None else MODEL_PROFILES[model_name][\"input_size\"]\n",
    "# √ñNEMLƒ∞: Artƒ±k batch_size parametresini ger√ßekten kullanƒ±yoruz\n",
    "    train_loader, val_loader, test_loader, full_train = make_dataloaders(\n",
    "        model_name=model_name,\n",
    "        input_size=input_size,\n",
    "        batch_size=batch_size,\n",
    "        aug_strength=aug_strength\n",
    "    )\n",
    "\n",
    "\n",
    "    model = build_model(model_name).to(DEVICE)\n",
    "    class_weights = compute_class_weights(full_train).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = make_optimizer(model, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "    history = History(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 7\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            model, train_loader, criterion,\n",
    "            optimizer=optimizer, scaler=scaler\n",
    "        )\n",
    "        va_loss, va_acc = run_one_epoch(\n",
    "            model, val_loader, criterion,\n",
    "            optimizer=None, scaler=None\n",
    "        )\n",
    "\n",
    "        history.train_loss.append(tr_loss)\n",
    "        history.val_loss.append(va_loss)\n",
    "        history.train_acc.append(tr_acc)\n",
    "        history.val_acc.append(va_acc)\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732f969",
   "metadata": {},
   "source": [
    "# üîé Random Search Yardƒ±mcƒ±larƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cf7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# üîé Random Search Yardƒ±mcƒ±larƒ±\n",
    "# =========================\n",
    "import math, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def sample_from(space: dict, rng: random.Random) -> dict:\n",
    "    \"\"\"\n",
    "    Hyperparam alanƒ±ndan tek bir √∂rnek se√ßer.\n",
    "    Not: rng dƒ±≈üarƒ±dan verilir; global random seed resetlerinden etkilenmez.\n",
    "    \n",
    "    Hyperparam alanƒ±ndan tek bir √∂rnek se√ßer.\n",
    "    space formatƒ± √∂rn:\n",
    "    {\n",
    "        \"lr\": {\"type\": \"loguniform\", \"low\": 1e-5, \"high\": 1e-2},\n",
    "        \"weight_decay\": {\"type\": \"loguniform\", \"low\": 1e-6, \"high\": 1e-3},\n",
    "        \"batch_size\": {\"type\": \"choice\", \"values\": [8, 16, 32]},\n",
    "        \"model_name\": {\"type\": \"choice\", \"values\": [\"efficientnet_v2_m\", \"resnet50\"]}\n",
    "    }\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, cfg in space.items():\n",
    "        t = cfg[\"type\"]\n",
    "        if t == \"choice\":\n",
    "            out[k] = rng.choice(cfg[\"values\"])\n",
    "        elif t == \"uniform\":\n",
    "            lo, hi = float(cfg[\"low\"]), float(cfg[\"high\"])\n",
    "            out[k] = rng.random() * (hi - lo) + lo\n",
    "        elif t == \"loguniform\":\n",
    "            lo, hi = math.log(float(cfg[\"low\"])), math.log(float(cfg[\"high\"]))\n",
    "            out[k] = math.exp(rng.random() * (hi - lo) + lo)\n",
    "        else:\n",
    "            raise ValueError(f\"Bilinmeyen t√ºr: {t}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def try_set_seed(seed: int):\n",
    "    try:\n",
    "        set_seed(seed)  # Notebook'ta varsa kullan\n",
    "    except Exception:\n",
    "        # Yoksa sessizce ge√ß\n",
    "        pass\n",
    "\n",
    "def run_one_trial(cfg,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  model_name,\n",
    "                  metric=\"val_acc\",\n",
    "                  seed=42,\n",
    "                  aug_strength: float = 0.0):\n",
    "    try_set_seed(seed)\n",
    "\n",
    "    model, history,_, test_loader = train_model_one_run(\n",
    "        model_name=model_name,\n",
    "        max_epochs=max_epochs,\n",
    "        lr=cfg[\"lr\"],\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        aug_strength=aug_strength\n",
    "    )\n",
    "\n",
    "    trial_name = (\n",
    "        f\"{model_name}_lr{cfg['lr']:.2e}\"\n",
    "        f\"_wd{cfg['weight_decay']:.2e}\"\n",
    "        f\"_bs{cfg['batch_size']}\" f\"_aug{aug_strength:.2f}\"\n",
    "    )\n",
    "    trial_dir = output_root / trial_name\n",
    "    ensure_dir(trial_dir)\n",
    "\n",
    "    try:\n",
    "        plot_and_save_history(history, trial_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Plot failed:\", e)\n",
    "\n",
    "    # Metric se√ßimi\n",
    "    if metric == \"val_loss\" and history.val_loss:\n",
    "        metric_value = min(history.val_loss)\n",
    "    elif metric == \"val_acc\" and history.val_acc:\n",
    "        metric_value = max(history.val_acc)\n",
    "    else:\n",
    "        print(f\"[WARN] Metric {metric} desteklenmiyor, val_acc kullanƒ±lacak.\")\n",
    "        metric_value = max(history.val_acc) if history.val_acc else float(\"-inf\")\n",
    "\n",
    "    return metric_value, trial_dir\n",
    "\n",
    "\n",
    "def random_search(space,\n",
    "                  n_trials,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  metric=\"val_acc\",\n",
    "                  greater_is_better=True,\n",
    "                  model_name=None,\n",
    "                  aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Tek bir model (model_name) i√ßin random search yapar.\n",
    "    space: lr, weight_decay, batch_size gibi hiperparametre aralƒ±ƒüƒ±\n",
    "    \"\"\"\n",
    "    ensure_dir(output_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    results = []\n",
    "    sampler_rng = random.Random(12345)  # sadece hyperparam √∂rneklemek i√ßin ayrƒ± RNG\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # Hiperparametreleri random se√ß\n",
    "        cfg = sample_from(space, sampler_rng)\n",
    "\n",
    "        # Bir trial ko≈ü\n",
    "        metric_value, trial_dir = run_one_trial(\n",
    "            cfg=cfg,\n",
    "            max_epochs=max_epochs,\n",
    "            output_root=output_root,\n",
    "            model_name=model_name,\n",
    "            metric=metric,\n",
    "            seed=42 + i,\n",
    "            aug_strength=aug_strength\n",
    "        )\n",
    "\n",
    "        # Sonucu kaydet\n",
    "        row = {\n",
    "            **cfg,\n",
    "            \"metric_name\": metric,\n",
    "            \"metric_value\": metric_value,\n",
    "            \"trial_dir\": str(trial_dir),\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "    # En iyi sonucu se√ß\n",
    "    if greater_is_better:\n",
    "        best_row = max(results, key=lambda r: r[\"metric_value\"])\n",
    "    else:\n",
    "        best_row = min(results, key=lambda r: r[\"metric_value\"])\n",
    "\n",
    "    best_cfg = {k: best_row[k] for k in space.keys()}\n",
    "\n",
    "    # Pandas DataFrame'e √ßevir\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV olarak kaydet\n",
    "    csv_path = output_root / \"random_search_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Random search sonu√ßlarƒ± kaydedildi:\", csv_path)\n",
    "\n",
    "    return best_cfg, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98561d",
   "metadata": {},
   "source": [
    "# üöÄ Random Search'i √áalƒ±≈ütƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35619748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n",
      "Random search sonu√ßlarƒ± kaydedildi: sweeps/random_search/random_search_results.csv\n",
      "\n",
      "Se√ßilen model: custom_msaf_effb0\n",
      "En iyi konfig√ºrasyon:\n",
      "{'lr': 0.00010765096851001565, 'weight_decay': 1.072772194472807e-06, 'batch_size': 24}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>trial_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr1.08e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr1.46e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr2.07e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr1.18e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr6.93e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr1.02e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr6.68e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr3.44e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr2.52e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr3.28e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr2.33e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr4.13e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr9.98e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.998018</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr1.31e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.997027</td>\n",
       "      <td>sweeps/random_search/custom_msaf_effb0_lr8.12e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lr  weight_decay  batch_size metric_name  metric_value  \\\n",
       "0   0.000108      0.000001          24     val_acc      1.000000   \n",
       "3   0.001464      0.000006          16     val_acc      1.000000   \n",
       "5   0.000207      0.000161          16     val_acc      1.000000   \n",
       "7   0.001184      0.000009          32     val_acc      1.000000   \n",
       "8   0.000069      0.000799          32     val_acc      1.000000   \n",
       "9   0.000010      0.000662          16     val_acc      1.000000   \n",
       "12  0.000668      0.000005          24     val_acc      1.000000   \n",
       "14  0.000034      0.000255          16     val_acc      1.000000   \n",
       "2   0.000252      0.000003           8     val_acc      0.999009   \n",
       "4   0.000328      0.000004           8     val_acc      0.999009   \n",
       "6   0.000023      0.000144          16     val_acc      0.999009   \n",
       "11  0.000413      0.000002          16     val_acc      0.999009   \n",
       "13  0.000998      0.000005          32     val_acc      0.999009   \n",
       "1   0.001313      0.000624          24     val_acc      0.998018   \n",
       "10  0.000812      0.000003           8     val_acc      0.997027   \n",
       "\n",
       "                                            trial_dir  \n",
       "0   sweeps/random_search/custom_msaf_effb0_lr1.08e...  \n",
       "3   sweeps/random_search/custom_msaf_effb0_lr1.46e...  \n",
       "5   sweeps/random_search/custom_msaf_effb0_lr2.07e...  \n",
       "7   sweeps/random_search/custom_msaf_effb0_lr1.18e...  \n",
       "8   sweeps/random_search/custom_msaf_effb0_lr6.93e...  \n",
       "9   sweeps/random_search/custom_msaf_effb0_lr1.02e...  \n",
       "12  sweeps/random_search/custom_msaf_effb0_lr6.68e...  \n",
       "14  sweeps/random_search/custom_msaf_effb0_lr3.44e...  \n",
       "2   sweeps/random_search/custom_msaf_effb0_lr2.52e...  \n",
       "4   sweeps/random_search/custom_msaf_effb0_lr3.28e...  \n",
       "6   sweeps/random_search/custom_msaf_effb0_lr2.33e...  \n",
       "11  sweeps/random_search/custom_msaf_effb0_lr4.13e...  \n",
       "13  sweeps/random_search/custom_msaf_effb0_lr9.98e...  \n",
       "1   sweeps/random_search/custom_msaf_effb0_lr1.31e...  \n",
       "10  sweeps/random_search/custom_msaf_effb0_lr8.12e...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# üöÄ Random Search'i √áalƒ±≈ütƒ±r\n",
    "# =========================\n",
    "\n",
    "# 1) Arama alanƒ±nƒ± tanƒ±mla (artƒ±k model_name yok, sadece hiperparametreler var)\n",
    "search_space = {\n",
    "    \"lr\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-5,\n",
    "        \"high\": 3e-3,\n",
    "    },\n",
    "    \"weight_decay\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-6,\n",
    "        \"high\": 1e-3,\n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [8, 16, 24, 32],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 2) S√ºp√ºrme ayarlarƒ±\n",
    "N_TRIALS   = 15          # Ka√ß deneme yapƒ±lacak\n",
    "MAX_EPOCHS = 10           # Her denemenin epoch sayƒ±sƒ±\n",
    "METRIC     = \"val_acc\"   # Artƒ±k AUC deƒüil, validation accuracy kullanƒ±yoruz\n",
    "HIGHER_BETTER = True     # val_acc i√ßin b√ºy√ºk olan daha iyidir\n",
    "OUTPUT_ROOT = \"./sweeps/random_search\"\n",
    "\n",
    "# 3) √áalƒ±≈ütƒ±r\n",
    "best, df = random_search(\n",
    "    space=search_space,\n",
    "    n_trials=N_TRIALS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    metric=METRIC,\n",
    "    greater_is_better=HIGHER_BETTER,\n",
    "    model_name=TARGET_MODEL,   # >>> BURASI √ñNEMLƒ∞: sadece se√ßtiƒüin modeli geziyoruz\n",
    "    aug_strength=AUG_STRENGTH,\n",
    ")\n",
    "\n",
    "print(\"\\nSe√ßilen model:\", TARGET_MODEL)\n",
    "print(\"En iyi konfig√ºrasyon:\")\n",
    "print(best)\n",
    "\n",
    "# DataFrame'i g√∂r√ºnt√ºle\n",
    "try:\n",
    "    import IPython\n",
    "    from IPython.display import display\n",
    "    display(df.sort_values(\"metric_value\", ascending=not HIGHER_BETTER))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e821e8",
   "metadata": {},
   "source": [
    "## üìä Deƒüerlendirme + Grafik ve Tablo Kaydƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5103595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def plot_and_save_history(hist, out_dir: str):\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_acc)+1), hist.train_acc, label=\"train_acc\")\n",
    "    plt.plot(range(1, len(hist.val_acc)+1),   hist.val_acc,   label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"accuracy.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_loss)+1), hist.train_loss, label=\"train_loss\")\n",
    "    plt.plot(range(1, len(hist.val_loss)+1),   hist.val_loss,   label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_save(model, test_loader, out_dir: str, model_name: str, threshold: float = 0.5):\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_prob, y_pred = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            logits = model(images)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]          # tensor\n",
    "            preds = (probs >= threshold).long()                 # ‚úÖ threshold ile karar\n",
    "\n",
    "            probs = probs.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "\n",
    "            y_true.extend(labels.numpy().tolist())     # istersen alttaki \"g√ºvenli\" versiyona ge√ß\n",
    "            y_prob.extend(probs.tolist())              # ‚úÖ probs zaten numpy\n",
    "            y_pred.extend(preds.tolist())              # ‚úÖ preds zaten numpy\n",
    "\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # === Hata Matrisi ===\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, CLASS_NAMES, rotation=45)\n",
    "    plt.yticks(tick_marks, CLASS_NAMES)\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_matrix.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # === ROC Eƒürisi ===\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"roc_curve.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # === Metrikler ===\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    recall    = recall_score(y_true, y_pred,    pos_label=1, zero_division=0)  # sensitivity\n",
    "    f1        = f1_score(y_true, y_pred,        pos_label=1, zero_division=0)\n",
    "    kappa     = cohen_kappa_score(y_true, y_pred)\n",
    "    acc       = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Tabloyu PNG olarak kaydet\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    cell_text = [[f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{kappa:.4f}\", f\"{acc:.4f}\"]]\n",
    "    col_labels = [\"Precision\", \"Sensitivity (Recall)\", \"F1-Score\", \"Cohen's Kappa\", \"Accuracy\"]\n",
    "    the_table = ax.table(cellText=cell_text, colLabels=col_labels, loc='center')\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(11)\n",
    "    the_table.scale(1.2, 1.6)\n",
    "    plt.title(\"Deƒüerlendirme Metrikleri\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, \"metrics_table.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Ayrƒ±ca CSV ve classification_report da kaydedelim\n",
    "    import csv\n",
    "    with open(os.path.join(out_dir, \"metrics.csv\"), \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"metric\", \"value\"])\n",
    "        writer.writerow([\"precision\", precision])\n",
    "        writer.writerow([\"recall_sensitivity\", recall])\n",
    "        writer.writerow([\"f1\", f1])\n",
    "        writer.writerow([\"kappa\", kappa])\n",
    "        writer.writerow([\"auc\", roc_auc])\n",
    "        writer.writerow([\"accuracy\", acc])\n",
    "\n",
    "    with open(os.path.join(out_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "        f.write(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall_sensitivity\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": kappa,\n",
    "        \"auc\": roc_auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7a7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        logits = model(images)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # tumor olasƒ±lƒ±ƒüƒ±\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_prob.append(probs.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(y_true), np.concatenate(y_prob)\n",
    "\n",
    "def select_threshold_on_val(y_true, y_prob, objective=\"f1\"):\n",
    "    thresholds = np.linspace(0.0, 1.0, 1001)\n",
    "    best_t, best_score = 0.5, -1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "\n",
    "        if objective == \"f1\":\n",
    "            score = f1_score(y_true, y_pred, pos_label=1)\n",
    "        elif objective == \"bal\":\n",
    "            score = balanced_accuracy_score(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"objective must be 'f1' or 'bal'\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_t = float(t)\n",
    "\n",
    "    return best_t, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cce677",
   "metadata": {},
   "source": [
    "# üèÅ En iyi konfig√ºrasyonla yeniden-eƒüitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe00868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search en iyi hiperparametreler: {'lr': 0.00010765096851001565, 'weight_decay': 1.072772194472807e-06, 'batch_size': 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/tmp/ipykernel_47451/340905458.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_47451/340905458.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 24\n",
      "VAL'dan se√ßilen en iyi threshold: 0.003 obj_score: 1.0\n",
      "\n",
      "Final eƒüitim tamamlandƒ±.\n",
      "Metrikler: {'precision': 0.9990234375, 'recall_sensitivity': 0.6181268882175227, 'f1': 0.7637178051511758, 'kappa': 0.613330549164214, 'auc': 0.9355656050425571, 'accuracy': 0.8054103904088533, 'confusion_matrix': [[1597, 1], [632, 1023]]}\n",
      "Sonu√ß klas√∂r√º: results/custom_msaf_effb0_final\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Search en iyi hiperparametreler:\", best)\n",
    "\n",
    "FINAL_EPOCHS = 100  # ƒ∞stersen artƒ±r\n",
    "\n",
    "# Reprod√ºksiyon i√ßin seed\n",
    "try_set_seed(42)\n",
    "\n",
    "# Eƒüitim\n",
    "model, history, val_loader, test_loader = train_model_one_run(\n",
    "    model_name=TARGET_MODEL,\n",
    "    max_epochs=FINAL_EPOCHS,\n",
    "    lr=best[\"lr\"],\n",
    "    weight_decay=best[\"weight_decay\"],\n",
    "    batch_size=best[\"batch_size\"],\n",
    "    aug_strength=AUG_STRENGTH,\n",
    ")\n",
    "\n",
    "# √áƒ±ktƒ±larƒ± kaydet\n",
    "final_out_dir = os.path.join(\"results\", f\"{TARGET_MODEL}_final\")\n",
    "os.makedirs(final_out_dir, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(final_out_dir, \"best.pt\"))\n",
    "plot_and_save_history(history, final_out_dir)\n",
    "\n",
    "# ‚úÖ 1) VAL'dan threshold se√ß (test'e dokunma)\n",
    "y_val, p_val = collect_probs(model, val_loader, DEVICE)\n",
    "best_thr, best_obj = select_threshold_on_val(y_val, p_val, objective=\"f1\")\n",
    "print(\"VAL'dan se√ßilen en iyi threshold:\", best_thr, \"obj_score:\", best_obj)\n",
    "\n",
    "with open(os.path.join(final_out_dir, \"best_threshold.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"best_thr={best_thr}\\nobjective=f1\\nobj_score={best_obj}\\n\")\n",
    "\n",
    "# ‚úÖ 2) TEST'i bu threshold ile deƒüerlendir\n",
    "summary = evaluate_and_save(model, test_loader, final_out_dir, TARGET_MODEL, threshold=best_thr)\n",
    "\n",
    "print(\"\\nFinal eƒüitim tamamlandƒ±.\")\n",
    "print(\"Metrikler:\", summary)\n",
    "print(\"Sonu√ß klas√∂r√º:\", final_out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
