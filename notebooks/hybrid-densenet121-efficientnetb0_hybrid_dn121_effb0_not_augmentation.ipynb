{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ad5794",
   "metadata": {},
   "source": [
    "# üß† Brain MRI ƒ∞kili Sƒ±nƒ±flandƒ±rma ‚Äî Modeli Tek Tek Se√ßerek Eƒüitim\n",
    "\n",
    "\n",
    "Bu defter, **sadece ikili sƒ±nƒ±flandƒ±rma** (labels: **`tumor`** ve **`no_tumor`**) i√ßin d√ºzenlendi.\n",
    "A≈üaƒüƒ±daki h√ºcreler ile **modeli tek tek kendiniz se√ßip** eƒüitebilir; her eƒüitim i√ßin **ayrƒ± ayrƒ±**:\n",
    "- Accuracy grafiƒüi,\n",
    "- Loss grafiƒüi,\n",
    "- **Hata matrisi (Confusion Matrix)**,\n",
    "- **ROC eƒürisi (AUC ile)**,\n",
    "- ve **tablo halinde Sensitivity (Recall), Precision, F1, Cohen‚Äôs Kappa**\n",
    "\n",
    "olu≈üturup **PNG olarak kaydedebilirsiniz**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441cfc8",
   "metadata": {},
   "source": [
    "## üì¶ Kurulumlar ve K√ºt√ºphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e670597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install -q timm scikit-learn torchmetrics\n",
    "\n",
    "import os, math, time, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm  # √ßok sayƒ±da SOTA mimari\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reprod√ºksiyon\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# DataLoader i≈ü√ßileri i√ßin deterministik davranƒ±≈ü\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# (Opsiyonel) PyTorch 2.x‚Äôte matmul hesaplarƒ±nƒ± ‚Äúhigh‚Äù hassasiyete √ßekerek olasƒ± hƒ±z optimizasyonu deniyor; desteklenmezse sessizce ge√ßiyor.\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 1) # DataLoader i√ßin en fazla 4 olmak √ºzere CPU √ßekirdek sayƒ±sƒ± kadar i≈ü√ßi belirliyor.\n",
    "BATCH_SIZE = 32\n",
    "VAL_RATIO = 0.1   # verinin %10‚Äôu validasyon i√ßin ayrƒ±lacak.\n",
    "MAX_EPOCHS_DEFAULT = 50 \n",
    "\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb666295",
   "metadata": {},
   "source": [
    "## üß≠ Yollar ve Temel Ayarlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d029f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß Dƒ∞Zƒ∞N YAPISI\n",
    "# DATA_ROOT:\n",
    "#   ‚îú‚îÄ‚îÄ Training\n",
    "#   ‚îÇ    ‚îú‚îÄ‚îÄ tumor\n",
    "#   ‚îÇ    ‚îî‚îÄ‚îÄ no_tumor\n",
    "#   ‚îî‚îÄ‚îÄ Testing\n",
    "#        ‚îú‚îÄ‚îÄ tumor\n",
    "#        ‚îî‚îÄ‚îÄ no_tumor\n",
    "\n",
    "DATA_ROOT = \"Dataset\"  # Veri seti k√∂k dizin.\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"Training\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"Testing\")\n",
    "\n",
    "# Etiket sƒ±rasƒ± sabitliyorum: 0 = no_tumor, 1 = tumor\n",
    "CLASS_NAMES = [\"no_tumor\", \"tumor\"]\n",
    "POS_LABEL = 1  # ROC/metrics i√ßin pozitif sƒ±nƒ±f (tumor)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True) # √áƒ±ktƒ±lar i√ßin results klas√∂r√º yoksa olu≈üturuluyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadcc60",
   "metadata": {},
   "source": [
    "## üß© Model Profilleri (Input Size) ve Tekli Se√ßim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709ae705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: hybrid_dn121_effb0 Input: 256\n"
     ]
    }
   ],
   "source": [
    "# Giri≈ü boyutu √∂nerileri\n",
    "MODEL_PROFILES = {\n",
    "    \"resnet34\": {\"input_size\": 224},\n",
    "    \"resnet50\": {\"input_size\": 224},\n",
    "    \"densenet121\": {\"input_size\": 224},\n",
    "    \"inception_v3\": {\"input_size\": 299},\n",
    "    \"efficientnet_b0\": {\"input_size\": 224},\n",
    "    \"mobilenetv2_100\": {\"input_size\": 224},\n",
    "    \"convnext_tiny\": {\"input_size\": 224},\n",
    "\n",
    "    # ‚úÖ HYBRID\n",
    "    \"hybrid_dn121_effb0\": {\"input_size\": 256},\n",
    "}\n",
    "\n",
    "ALL_MODELS = list(MODEL_PROFILES.keys())\n",
    "\n",
    "# ‚úÖ Senin kararƒ±n: dataset karƒ±≈üƒ±k olduƒüu i√ßin her ≈üeyi 256'ya sabitle\n",
    "OVERRIDE_INPUT_SIZE = 256\n",
    "\n",
    "# Buradan TEK bir modeli se√ßin\n",
    "SELECT_ONE_MODEL = \"hybrid_dn121_effb0\"  # <- hibrit modeli se√ßtik\n",
    "TARGET_MODEL = SELECT_ONE_MODEL\n",
    "\n",
    "INPUT_SIZE = OVERRIDE_INPUT_SIZE if OVERRIDE_INPUT_SIZE is not None else MODEL_PROFILES[SELECT_ONE_MODEL][\"input_size\"]\n",
    "print(\"Selected:\", SELECT_ONE_MODEL, \"Input:\", INPUT_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ecac0",
   "metadata": {},
   "source": [
    "## üñºÔ∏è D√∂n√º≈ü√ºmler (Pad + Resize + Normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2710f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "\n",
    "# Model bazlƒ± mean/std cache\n",
    "_MEAN_STD_CACHE = {}\n",
    "\n",
    "# ‚úÖ Hibrit i√ßin mean/std hangi backbone'dan alƒ±nacak?\n",
    "_HYBRID_MEANSTD_SOURCE = {\n",
    "    \"hybrid_dn121_effb0\": \"densenet121\"  # ikisi de ImageNet normalize kullandƒ±ƒüƒ± i√ßin fark etmez\n",
    "}\n",
    "\n",
    "def get_mean_std_from_timm(model_name: str):\n",
    "    \"\"\"\n",
    "    timm modelinin default_cfg/pretrained_cfg i√ßinden mean/std √ßeker.\n",
    "    Hibrit model adƒ±nƒ± desteklemek i√ßin backbone'a map edilir.\n",
    "    \"\"\"\n",
    "    name = _HYBRID_MEANSTD_SOURCE.get(model_name, model_name)\n",
    "\n",
    "    if name in _MEAN_STD_CACHE:\n",
    "        return _MEAN_STD_CACHE[name]\n",
    "\n",
    "    m = timm.create_model(name, pretrained=False, num_classes=2)\n",
    "    cfg = resolve_data_config({}, model=m)\n",
    "\n",
    "    mean, std = cfg[\"mean\"], cfg[\"std\"]\n",
    "    _MEAN_STD_CACHE[name] = (mean, std)\n",
    "    del m\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            return img\n",
    "        size = max(w, h)\n",
    "        pad_left = (size - w) // 2\n",
    "        pad_top = (size - h) // 2\n",
    "        pad_right = size - w - pad_left\n",
    "        pad_bottom = size - h - pad_top\n",
    "        return transforms.functional.pad(\n",
    "            img, (pad_left, pad_top, pad_right, pad_bottom), fill=0\n",
    "        )\n",
    "\n",
    "\n",
    "def build_transforms(model_name: str, input_size: int, aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Augmentasyonsuz transform:\n",
    "    pad + resize + ToTensor + (MODEL'E UYGUN Normalize)\n",
    "    \"\"\"\n",
    "    mean, std = get_mean_std_from_timm(model_name)\n",
    "\n",
    "    base = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    train_tfms = base\n",
    "    eval_tfms = base\n",
    "    return train_tfms, eval_tfms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59819689",
   "metadata": {},
   "source": [
    "## üì• Dataset & DataLoader'lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9673c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def make_dataloaders(model_name: str, input_size: int, batch_size: int = BATCH_SIZE, val_ratio: float = VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Train/Val/test DataLoader kurulumunu doƒüru transform'larla yapar.\n",
    "    - Train: train_tfms\n",
    "    - Val/Test: eval_tfms\n",
    "    Ayrƒ±ca train subset'i geri d√∂nd√ºr√ºr (sƒ±nƒ±f aƒüƒ±rlƒ±klarƒ± i√ßin).\n",
    "    \"\"\"\n",
    "    train_tfms, eval_tfms = build_transforms(model_name, input_size)\n",
    "\n",
    "    # 1) Stratified split i√ßin base dataset + etiketler\n",
    "    base_ds = datasets.ImageFolder(TRAIN_DIR)  # transform YOK\n",
    "    y = np.array(base_ds.targets)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)\n",
    "    train_indices, val_indices = next(sss.split(np.zeros(len(y)), y))\n",
    "\n",
    "    # 2) Ayrƒ± g√∂r√ºn√ºmler: train vs val/test i√ßin farklƒ± transform\n",
    "    train_view = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "    val_view   = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
    "    test_ds    = datasets.ImageFolder(TEST_DIR,  transform=eval_tfms)\n",
    "\n",
    "    # 3) ƒ∞ndeksleri Subset'lere uygula\n",
    "    train_ds = Subset(train_view, train_indices.tolist())\n",
    "    val_ds   = Subset(val_view,   val_indices.tolist())\n",
    "\n",
    "    # 4) (Opsiyonel) Sƒ±nƒ±f sƒ±rasƒ± kontrol√º\n",
    "    expected = [\"no_tumor\", \"tumor\"]\n",
    "    assert train_view.classes == expected, f\"Sƒ±nƒ±f sƒ±rasƒ± {train_view.classes} beklenen {expected} deƒüil!\"\n",
    "\n",
    "    # 5) DataLoader'lar (deterministik workers)\n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    pin = (DEVICE.type == \"cuda\")\n",
    "    common = dict(num_workers=NUM_WORKERS, pin_memory=pin, worker_init_fn=seed_worker, persistent_workers=bool(NUM_WORKERS))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  generator=gen, **common)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076ecb9",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model & Kayƒ±p Fonksiyonu & Optimizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7da69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDN121_EffB0(nn.Module):\n",
    "    \"\"\"\n",
    "    DenseNet121 + EfficientNetB0 hibrit:\n",
    "    - ƒ∞ki backbone'dan pooled feature √ßƒ±karƒ±r (num_classes=0, global_pool='avg')\n",
    "    - Concatenate + k√º√ß√ºk bir MLP head ile 2 sƒ±nƒ±f logits √ºretir\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, head_dim=256, dropout=0.2, freeze_backbones=False):\n",
    "        super().__init__()\n",
    "        self.bb1 = timm.create_model(\"densenet121\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.bb2 = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        if freeze_backbones:\n",
    "            for p in self.bb1.parameters(): p.requires_grad = False\n",
    "            for p in self.bb2.parameters(): p.requires_grad = False\n",
    "\n",
    "        dim1 = getattr(self.bb1, \"num_features\")\n",
    "        dim2 = getattr(self.bb2, \"num_features\")\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(dim1 + dim2, head_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_dim, num_classes)   # ‚úÖ logits (softmax yok)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.bb1(x)\n",
    "        f2 = self.bb2(x)\n",
    "        feats = torch.cat([f1, f2], dim=1)\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "def build_model(model_name: str, num_classes: int = 2):\n",
    "    if model_name == \"hybrid_dn121_effb0\":\n",
    "        # freeze_backbones=True dersen overfitting azalƒ±r ama tavan performansƒ± d√º≈üebilir\n",
    "        return HybridDN121_EffB0(num_classes=num_classes, head_dim=256, dropout=0.2, freeze_backbones=False)\n",
    "\n",
    "    # normal timm modeller\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    ImageFolder ya da Subset(ImageFolder) kabul eder.\n",
    "    \"\"\"\n",
    "    # Subset ise hedef etiketleri indekslerden topla\n",
    "    if isinstance(dataset, Subset):\n",
    "        base = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "        targets = getattr(base, \"targets\", None)\n",
    "        if targets is None:\n",
    "            raise ValueError(\"Temel dataset'te 'targets' bulunamadƒ±.\")\n",
    "        labels = [targets[i] for i in indices]\n",
    "    else:\n",
    "        labels = list(getattr(dataset, \"targets\", []))\n",
    "\n",
    "    n0 = sum(1 for y in labels if y == 0)\n",
    "    n1 = sum(1 for y in labels if y == 1)\n",
    "    total = max(1, n0 + n1)\n",
    "    # Basit ters frekans aƒüƒ±rlƒ±klandƒ±rmasƒ±\n",
    "    w0 = total / (2.0 * max(1, n0))\n",
    "    w1 = total / (2.0 * max(1, n1))\n",
    "    return torch.tensor([w0, w1], dtype=torch.float)\n",
    "    \n",
    "def make_optimizer(model, lr=3e-4, weight_decay=1e-4):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600cff1",
   "metadata": {},
   "source": [
    "## üîÅ Eƒüitim & Doƒürulama D√∂ng√ºs√º (History ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7926b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class History:\n",
    "    train_loss: list\n",
    "    val_loss: list\n",
    "    train_acc: list\n",
    "    val_acc: list\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def run_one_epoch(model, loader, criterion, optimizer=None, scaler: GradScaler = None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler is not None and DEVICE.type == \"cuda\":\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "def train_model_one_run(model_name: str,\n",
    "                        max_epochs: int = MAX_EPOCHS_DEFAULT,\n",
    "                        lr: float = 3e-4,\n",
    "                        weight_decay: float = 1e-4,\n",
    "                        batch_size: int = BATCH_SIZE):\n",
    "    input_size = MODEL_PROFILES[model_name][\"input_size\"]\n",
    "\n",
    "    # √ñNEMLƒ∞: Artƒ±k batch_size parametresini ger√ßekten kullanƒ±yoruz\n",
    "    train_loader, val_loader, test_loader, full_train = make_dataloaders(\n",
    "        model_name=model_name,\n",
    "        input_size=input_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "    model = build_model(model_name).to(DEVICE)\n",
    "    class_weights = compute_class_weights(full_train).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = make_optimizer(model, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "    history = History(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 7\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            model, train_loader, criterion,\n",
    "            optimizer=optimizer, scaler=scaler\n",
    "        )\n",
    "        va_loss, va_acc = run_one_epoch(\n",
    "            model, val_loader, criterion,\n",
    "            optimizer=None, scaler=None\n",
    "        )\n",
    "\n",
    "        history.train_loss.append(tr_loss)\n",
    "        history.val_loss.append(va_loss)\n",
    "        history.train_acc.append(tr_acc)\n",
    "        history.val_acc.append(va_acc)\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732f969",
   "metadata": {},
   "source": [
    "# üîé Random Search Yardƒ±mcƒ±larƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4cf7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# üîé Random Search Yardƒ±mcƒ±larƒ±\n",
    "# =========================\n",
    "import math, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def sample_from(space: dict, rng: random.Random) -> dict:\n",
    "    \"\"\"\n",
    "    Hyperparam alanƒ±ndan tek bir √∂rnek se√ßer.\n",
    "    Not: rng dƒ±≈üarƒ±dan verilir; global random seed resetlerinden etkilenmez.\n",
    "    \n",
    "    Hyperparam alanƒ±ndan tek bir √∂rnek se√ßer.\n",
    "    space formatƒ± √∂rn:\n",
    "    {\n",
    "        \"lr\": {\"type\": \"loguniform\", \"low\": 1e-5, \"high\": 1e-2},\n",
    "        \"weight_decay\": {\"type\": \"loguniform\", \"low\": 1e-6, \"high\": 1e-3},\n",
    "        \"batch_size\": {\"type\": \"choice\", \"values\": [8, 16, 32]},\n",
    "        \"model_name\": {\"type\": \"choice\", \"values\": [\"efficientnet_v2_m\", \"resnet50\"]}\n",
    "    }\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, cfg in space.items():\n",
    "        t = cfg[\"type\"]\n",
    "        if t == \"choice\":\n",
    "            out[k] = rng.choice(cfg[\"values\"])\n",
    "        elif t == \"uniform\":\n",
    "            lo, hi = float(cfg[\"low\"]), float(cfg[\"high\"])\n",
    "            out[k] = rng.random() * (hi - lo) + lo\n",
    "        elif t == \"loguniform\":\n",
    "            lo, hi = math.log(float(cfg[\"low\"])), math.log(float(cfg[\"high\"]))\n",
    "            out[k] = math.exp(rng.random() * (hi - lo) + lo)\n",
    "        else:\n",
    "            raise ValueError(f\"Bilinmeyen t√ºr: {t}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def try_set_seed(seed: int):\n",
    "    try:\n",
    "        set_seed(seed)  # Notebook'ta varsa kullan\n",
    "    except Exception:\n",
    "        # Yoksa sessizce ge√ß\n",
    "        pass\n",
    "\n",
    "def run_one_trial(cfg,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  model_name,\n",
    "                  metric=\"val_acc\",\n",
    "                  seed=42):\n",
    "    try_set_seed(seed)\n",
    "\n",
    "    model, history,_, test_loader = train_model_one_run(\n",
    "        model_name=model_name,\n",
    "        max_epochs=max_epochs,\n",
    "        lr=cfg[\"lr\"],\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "        batch_size=cfg[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    trial_name = (\n",
    "        f\"{model_name}_lr{cfg['lr']:.2e}\"\n",
    "        f\"_wd{cfg['weight_decay']:.2e}\"\n",
    "        f\"_bs{cfg['batch_size']}\"\n",
    "    )\n",
    "    trial_dir = output_root / trial_name\n",
    "    ensure_dir(trial_dir)\n",
    "\n",
    "    try:\n",
    "        plot_and_save_history(history, trial_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Plot failed:\", e)\n",
    "\n",
    "    # Metric se√ßimi\n",
    "    if metric == \"val_loss\" and history.val_loss:\n",
    "        metric_value = min(history.val_loss)\n",
    "    elif metric == \"val_acc\" and history.val_acc:\n",
    "        metric_value = max(history.val_acc)\n",
    "    else:\n",
    "        print(f\"[WARN] Metric {metric} desteklenmiyor, val_acc kullanƒ±lacak.\")\n",
    "        metric_value = max(history.val_acc) if history.val_acc else float(\"-inf\")\n",
    "\n",
    "    return metric_value, trial_dir\n",
    "\n",
    "\n",
    "def random_search(space,\n",
    "                  n_trials,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  metric=\"val_acc\",\n",
    "                  greater_is_better=True,\n",
    "                  model_name=None):\n",
    "    \"\"\"\n",
    "    Tek bir model (model_name) i√ßin random search yapar.\n",
    "    space: lr, weight_decay, batch_size gibi hiperparametre aralƒ±ƒüƒ±\n",
    "    \"\"\"\n",
    "    ensure_dir(output_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    results = []\n",
    "    sampler_rng = random.Random(12345)  # sadece hyperparam √∂rneklemek i√ßin ayrƒ± RNG\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # Hiperparametreleri random se√ß\n",
    "        cfg = sample_from(space, sampler_rng)\n",
    "\n",
    "        # Bir trial ko≈ü\n",
    "        metric_value, trial_dir = run_one_trial(\n",
    "            cfg=cfg,\n",
    "            max_epochs=max_epochs,\n",
    "            output_root=output_root,\n",
    "            model_name=model_name,\n",
    "            metric=metric,\n",
    "            seed=42 + i\n",
    "        )\n",
    "\n",
    "        # Sonucu kaydet\n",
    "        row = {\n",
    "            **cfg,\n",
    "            \"metric_name\": metric,\n",
    "            \"metric_value\": metric_value,\n",
    "            \"trial_dir\": str(trial_dir),\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "    # En iyi sonucu se√ß\n",
    "    if greater_is_better:\n",
    "        best_row = max(results, key=lambda r: r[\"metric_value\"])\n",
    "    else:\n",
    "        best_row = min(results, key=lambda r: r[\"metric_value\"])\n",
    "\n",
    "    best_cfg = {k: best_row[k] for k in space.keys()}\n",
    "\n",
    "    # Pandas DataFrame'e √ßevir\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV olarak kaydet\n",
    "    csv_path = output_root / \"random_search_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Random search sonu√ßlarƒ± kaydedildi:\", csv_path)\n",
    "\n",
    "    return best_cfg, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98561d",
   "metadata": {},
   "source": [
    "# üöÄ Random Search'i √áalƒ±≈ütƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35619748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2e040da3a7435f973d8c0ec737d9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/32.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d02e23a14c4870b47b2d3706aa331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3562/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot failed: name 'plot_and_save_history' is not defined\n",
      "Random search sonu√ßlarƒ± kaydedildi: sweeps/random_search/random_search_results.csv\n",
      "\n",
      "Se√ßilen model: hybrid_dn121_effb0\n",
      "En iyi konfig√ºrasyon:\n",
      "{'lr': 6.934684230776975e-05, 'weight_decay': 0.000798817143317339, 'batch_size': 32}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>trial_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr6.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr1.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr1.31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr2.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr2.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr2.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.998018</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr3.28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.998018</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr1.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.998018</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr1.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.996036</td>\n",
       "      <td>sweeps/random_search/hybrid_dn121_effb0_lr1.46...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  weight_decay  batch_size metric_name  metric_value  \\\n",
       "8  0.000069      0.000799          32     val_acc      1.000000   \n",
       "0  0.000108      0.000001          24     val_acc      0.999009   \n",
       "1  0.001313      0.000624          24     val_acc      0.999009   \n",
       "2  0.000252      0.000003           8     val_acc      0.999009   \n",
       "5  0.000207      0.000161          16     val_acc      0.999009   \n",
       "6  0.000023      0.000144          16     val_acc      0.999009   \n",
       "4  0.000328      0.000004           8     val_acc      0.998018   \n",
       "7  0.001184      0.000009          32     val_acc      0.998018   \n",
       "9  0.000010      0.000662          16     val_acc      0.998018   \n",
       "3  0.001464      0.000006          16     val_acc      0.996036   \n",
       "\n",
       "                                           trial_dir  \n",
       "8  sweeps/random_search/hybrid_dn121_effb0_lr6.93...  \n",
       "0  sweeps/random_search/hybrid_dn121_effb0_lr1.08...  \n",
       "1  sweeps/random_search/hybrid_dn121_effb0_lr1.31...  \n",
       "2  sweeps/random_search/hybrid_dn121_effb0_lr2.52...  \n",
       "5  sweeps/random_search/hybrid_dn121_effb0_lr2.07...  \n",
       "6  sweeps/random_search/hybrid_dn121_effb0_lr2.33...  \n",
       "4  sweeps/random_search/hybrid_dn121_effb0_lr3.28...  \n",
       "7  sweeps/random_search/hybrid_dn121_effb0_lr1.18...  \n",
       "9  sweeps/random_search/hybrid_dn121_effb0_lr1.02...  \n",
       "3  sweeps/random_search/hybrid_dn121_effb0_lr1.46...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# üöÄ Random Search'i √áalƒ±≈ütƒ±r\n",
    "# =========================\n",
    "\n",
    "# 1) Arama alanƒ±nƒ± tanƒ±mla (artƒ±k model_name yok, sadece hiperparametreler var)\n",
    "search_space = {\n",
    "    \"lr\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-5,\n",
    "        \"high\": 3e-3,\n",
    "    },\n",
    "    \"weight_decay\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-6,\n",
    "        \"high\": 1e-3,\n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [8, 16, 24, 32],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 2) S√ºp√ºrme ayarlarƒ±\n",
    "N_TRIALS   = 10          # Ka√ß deneme yapƒ±lacak\n",
    "MAX_EPOCHS = 8           # Her denemenin epoch sayƒ±sƒ±\n",
    "METRIC     = \"val_acc\"   # Artƒ±k AUC deƒüil, validation accuracy kullanƒ±yoruz\n",
    "HIGHER_BETTER = True     # val_acc i√ßin b√ºy√ºk olan daha iyidir\n",
    "OUTPUT_ROOT = \"./sweeps/random_search\"\n",
    "\n",
    "# 3) √áalƒ±≈ütƒ±r\n",
    "best, df = random_search(\n",
    "    space=search_space,\n",
    "    n_trials=N_TRIALS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    metric=METRIC,\n",
    "    greater_is_better=HIGHER_BETTER,\n",
    "    model_name=TARGET_MODEL,   # >>> BURASI √ñNEMLƒ∞: sadece se√ßtiƒüin modeli geziyoruz\n",
    ")\n",
    "\n",
    "print(\"\\nSe√ßilen model:\", TARGET_MODEL)\n",
    "print(\"En iyi konfig√ºrasyon:\")\n",
    "print(best)\n",
    "\n",
    "# DataFrame'i g√∂r√ºnt√ºle\n",
    "try:\n",
    "    import IPython\n",
    "    from IPython.display import display\n",
    "    display(df.sort_values(\"metric_value\", ascending=not HIGHER_BETTER))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e821e8",
   "metadata": {},
   "source": [
    "## üìä Deƒüerlendirme + Grafik ve Tablo Kaydƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5103595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def plot_and_save_history(hist, out_dir: str):\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_acc)+1), hist.train_acc, label=\"train_acc\")\n",
    "    plt.plot(range(1, len(hist.val_acc)+1),   hist.val_acc,   label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"accuracy.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_loss)+1), hist.train_loss, label=\"train_loss\")\n",
    "    plt.plot(range(1, len(hist.val_loss)+1),   hist.val_loss,   label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_save(model, test_loader, out_dir: str, model_name: str, threshold: float = 0.5):\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_prob, y_pred = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            logits = model(images)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]          # tensor\n",
    "            preds = (probs >= threshold).long()                 # ‚úÖ threshold ile karar\n",
    "\n",
    "            probs = probs.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "\n",
    "            y_true.extend(labels.numpy().tolist())     # istersen alttaki \"g√ºvenli\" versiyona ge√ß\n",
    "            y_prob.extend(probs.tolist())              # ‚úÖ probs zaten numpy\n",
    "            y_pred.extend(preds.tolist())              # ‚úÖ preds zaten numpy\n",
    "\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # === Hata Matrisi ===\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, CLASS_NAMES, rotation=45)\n",
    "    plt.yticks(tick_marks, CLASS_NAMES)\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_matrix.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # === ROC Eƒürisi ===\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"roc_curve.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # === Metrikler ===\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    recall    = recall_score(y_true, y_pred,    pos_label=1, zero_division=0)  # sensitivity\n",
    "    f1        = f1_score(y_true, y_pred,        pos_label=1, zero_division=0)\n",
    "    kappa     = cohen_kappa_score(y_true, y_pred)\n",
    "    acc       = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Tabloyu PNG olarak kaydet\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    cell_text = [[f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{kappa:.4f}\", f\"{acc:.4f}\"]]\n",
    "    col_labels = [\"Precision\", \"Sensitivity (Recall)\", \"F1-Score\", \"Cohen's Kappa\", \"Accuracy\"]\n",
    "    the_table = ax.table(cellText=cell_text, colLabels=col_labels, loc='center')\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(11)\n",
    "    the_table.scale(1.2, 1.6)\n",
    "    plt.title(\"Deƒüerlendirme Metrikleri\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, \"metrics_table.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Ayrƒ±ca CSV ve classification_report da kaydedelim\n",
    "    import csv\n",
    "    with open(os.path.join(out_dir, \"metrics.csv\"), \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"metric\", \"value\"])\n",
    "        writer.writerow([\"precision\", precision])\n",
    "        writer.writerow([\"recall_sensitivity\", recall])\n",
    "        writer.writerow([\"f1\", f1])\n",
    "        writer.writerow([\"kappa\", kappa])\n",
    "        writer.writerow([\"auc\", roc_auc])\n",
    "        writer.writerow([\"accuracy\", acc])\n",
    "\n",
    "    with open(os.path.join(out_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "        f.write(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall_sensitivity\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": kappa,\n",
    "        \"auc\": roc_auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c7a7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        logits = model(images)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # tumor olasƒ±lƒ±ƒüƒ±\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_prob.append(probs.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(y_true), np.concatenate(y_prob)\n",
    "\n",
    "def select_threshold_on_val(y_true, y_prob, objective=\"f1\"):\n",
    "    thresholds = np.linspace(0.0, 1.0, 1001)\n",
    "    best_t, best_score = 0.5, -1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "\n",
    "        if objective == \"f1\":\n",
    "            score = f1_score(y_true, y_pred, pos_label=1)\n",
    "        elif objective == \"bal\":\n",
    "            score = balanced_accuracy_score(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"objective must be 'f1' or 'bal'\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_t = float(t)\n",
    "\n",
    "    return best_t, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cce677",
   "metadata": {},
   "source": [
    "# üèÅ En iyi konfig√ºrasyonla yeniden-eƒüitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe00868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3220/3757662839.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_3220/3757662839.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19\n",
      "VAL'dan se√ßilen en iyi threshold: 0.009000000000000001 obj_score: 1.0\n",
      "\n",
      "Final eƒüitim tamamlandƒ±.\n",
      "Metrikler: {'precision': 1.0, 'recall_sensitivity': 0.683987915407855, 'f1': 0.8123430211697166, 'kappa': 0.6801545573377605, 'auc': 0.9389005894830774, 'accuracy': 0.8392253304641869, 'confusion_matrix': [[1598, 0], [523, 1132]]}\n",
      "Sonu√ß klas√∂r√º: results/hybrid_dn121_effb0_final\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Search en iyi hiperparametreler:\", best)\n",
    "\n",
    "FINAL_EPOCHS = 50  # ƒ∞stersen artƒ±r\n",
    "\n",
    "# Reprod√ºksiyon i√ßin seed\n",
    "try_set_seed(42)\n",
    "\n",
    "# Eƒüitim\n",
    "model, history, val_loader, test_loader = train_model_one_run(\n",
    "    model_name=TARGET_MODEL,\n",
    "    max_epochs=FINAL_EPOCHS,\n",
    "    lr=best[\"lr\"],\n",
    "    weight_decay=best[\"weight_decay\"],\n",
    "    batch_size=best[\"batch_size\"],\n",
    ")\n",
    "\n",
    "# √áƒ±ktƒ±larƒ± kaydet\n",
    "final_out_dir = os.path.join(\"results\", f\"{TARGET_MODEL}_final\")\n",
    "os.makedirs(final_out_dir, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(final_out_dir, \"best.pt\"))\n",
    "plot_and_save_history(history, final_out_dir)\n",
    "\n",
    "# ‚úÖ 1) VAL'dan threshold se√ß (test'e dokunma)\n",
    "y_val, p_val = collect_probs(model, val_loader, DEVICE)\n",
    "best_thr, best_obj = select_threshold_on_val(y_val, p_val, objective=\"f1\")\n",
    "print(\"VAL'dan se√ßilen en iyi threshold:\", best_thr, \"obj_score:\", best_obj)\n",
    "\n",
    "with open(os.path.join(final_out_dir, \"best_threshold.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"best_thr={best_thr}\\nobjective=f1\\nobj_score={best_obj}\\n\")\n",
    "\n",
    "# ‚úÖ 2) TEST'i bu threshold ile deƒüerlendir\n",
    "summary = evaluate_and_save(model, test_loader, final_out_dir, TARGET_MODEL, threshold=best_thr)\n",
    "\n",
    "print(\"\\nFinal eƒüitim tamamlandƒ±.\")\n",
    "print(\"Metrikler:\", summary)\n",
    "print(\"Sonu√ß klas√∂r√º:\", final_out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
