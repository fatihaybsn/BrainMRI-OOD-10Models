{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ad5794",
   "metadata": {},
   "source": [
    "# üß† Brain MRI ƒ∞kili Sƒ±nƒ±flandƒ±rma ‚Äî Modeli Tek Tek Se√ßerek Eƒüitim\n",
    "\n",
    "\n",
    "Bu defter, **sadece ikili sƒ±nƒ±flandƒ±rma** (labels: **`tumor`** ve **`no_tumor`**) i√ßin d√ºzenlendi.\n",
    "A≈üaƒüƒ±daki h√ºcreler ile **modeli tek tek kendiniz se√ßip** eƒüitebilir; her eƒüitim i√ßin **ayrƒ± ayrƒ±**:\n",
    "- Accuracy grafiƒüi,\n",
    "- Loss grafiƒüi,\n",
    "- **Hata matrisi (Confusion Matrix)**,\n",
    "- **ROC eƒürisi (AUC ile)**,\n",
    "- ve **tablo halinde Sensitivity (Recall), Precision, F1, Cohen‚Äôs Kappa**\n",
    "\n",
    "olu≈üturup **PNG olarak kaydedebilirsiniz**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441cfc8",
   "metadata": {},
   "source": [
    "## üì¶ Kurulumlar ve K√ºt√ºphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e670597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install -q timm scikit-learn torchmetrics\n",
    "\n",
    "import os, math, time, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm  # √ßok sayƒ±da SOTA mimari\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reprod√ºksiyon\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# DataLoader i≈ü√ßileri i√ßin deterministik davranƒ±≈ü\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# (Opsiyonel) PyTorch 2.x‚Äôte matmul hesaplarƒ±nƒ± ‚Äúhigh‚Äù hassasiyete √ßekerek olasƒ± hƒ±z optimizasyonu deniyor; desteklenmezse sessizce ge√ßiyor.\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 1) # DataLoader i√ßin en fazla 4 olmak √ºzere CPU √ßekirdek sayƒ±sƒ± kadar i≈ü√ßi belirliyor.\n",
    "BATCH_SIZE = 32\n",
    "VAL_RATIO = 0.1   # verinin %10‚Äôu validasyon i√ßin ayrƒ±lacak.\n",
    "MAX_EPOCHS_DEFAULT = 50 \n",
    "\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb666295",
   "metadata": {},
   "source": [
    "## üß≠ Yollar ve Temel Ayarlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d029f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß Dƒ∞Zƒ∞N YAPISI\n",
    "# DATA_ROOT:\n",
    "#   ‚îú‚îÄ‚îÄ Training\n",
    "#   ‚îÇ    ‚îú‚îÄ‚îÄ tumor\n",
    "#   ‚îÇ    ‚îî‚îÄ‚îÄ no_tumor\n",
    "#   ‚îî‚îÄ‚îÄ Testing\n",
    "#        ‚îú‚îÄ‚îÄ tumor\n",
    "#        ‚îî‚îÄ‚îÄ no_tumor\n",
    "\n",
    "DATA_ROOT = \"Dataset\"  # Veri seti k√∂k dizin.\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"Training\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"Testing\")\n",
    "\n",
    "# Etiket sƒ±rasƒ± sabitliyorum: 0 = no_tumor, 1 = tumor\n",
    "CLASS_NAMES = [\"no_tumor\", \"tumor\"]\n",
    "POS_LABEL = 1  # ROC/metrics i√ßin pozitif sƒ±nƒ±f (tumor)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True) # √áƒ±ktƒ±lar i√ßin results klas√∂r√º yoksa olu≈üturuluyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadcc60",
   "metadata": {},
   "source": [
    "## üß© Model Profilleri (Input Size) ve Tekli Se√ßim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709ae705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giri≈ü boyutu √∂nerileri\n",
    "MODEL_PROFILES = {\n",
    "    \"resnet34\": {\"input_size\": 224},\n",
    "    \"resnet50\": {\"input_size\": 224},\n",
    "    \"densenet121\": {\"input_size\": 224},\n",
    "    \"inception_v3\": {\"input_size\": 299},\n",
    "    \"efficientnet_b0\": {\"input_size\": 224},\n",
    "    \"mobilenet_v2\": {\"input_size\": 224},\n",
    "}\n",
    "\n",
    "ALL_MODELS = list(MODEL_PROFILES.keys()) # Bu modelleri liste haline getiriyor.\n",
    "\n",
    "# Buradan TEK bir modeli se√ßin\n",
    "SELECT_ONE_MODEL = \"inception_v3\"  # <- deƒüi≈ütirebilirsiniz\n",
    "TARGET_MODEL = SELECT_ONE_MODEL\n",
    "INPUT_SIZE = MODEL_PROFILES[SELECT_ONE_MODEL][\"input_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ecac0",
   "metadata": {},
   "source": [
    "## üñºÔ∏è D√∂n√º≈ü√ºmler (Pad + Resize + Normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2710f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            return img\n",
    "        size = max(w, h)\n",
    "        pad_left = (size - w) // 2\n",
    "        pad_top = (size - h) // 2\n",
    "        pad_right = size - w - pad_left\n",
    "        pad_bottom = size - h - pad_top\n",
    "        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "def build_transforms(input_size: int, aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Augmentasyonsuz transform:\n",
    "    Sadece pad + resize + ToTensor + Normalize.\n",
    "    Train ve eval aynƒ±.\n",
    "    \"\"\"\n",
    "    base = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    train_tfms = base\n",
    "    eval_tfms = base\n",
    "\n",
    "    return train_tfms, eval_tfms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59819689",
   "metadata": {},
   "source": [
    "## üì• Dataset & DataLoader'lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9673c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def make_dataloaders(input_size: int, batch_size: int = BATCH_SIZE, val_ratio: float = VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Train/Val/test DataLoader kurulumunu doƒüru transform'larla yapar.\n",
    "    - Train: train_tfms\n",
    "    - Val/Test: eval_tfms\n",
    "    Ayrƒ±ca train subset'i geri d√∂nd√ºr√ºr (sƒ±nƒ±f aƒüƒ±rlƒ±klarƒ± i√ßin).\n",
    "    \"\"\"\n",
    "    train_tfms, eval_tfms = build_transforms(input_size)\n",
    "\n",
    "    # 1) Stratified split i√ßin base dataset + etiketler\n",
    "    base_ds = datasets.ImageFolder(TRAIN_DIR)  # transform YOK\n",
    "    y = np.array(base_ds.targets)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)\n",
    "    train_indices, val_indices = next(sss.split(np.zeros(len(y)), y))\n",
    "\n",
    "    # 2) Ayrƒ± g√∂r√ºn√ºmler: train vs val/test i√ßin farklƒ± transform\n",
    "    train_view = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "    val_view   = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
    "    test_ds    = datasets.ImageFolder(TEST_DIR,  transform=eval_tfms)\n",
    "\n",
    "    # 3) ƒ∞ndeksleri Subset'lere uygula\n",
    "    train_ds = Subset(train_view, train_indices.tolist())\n",
    "    val_ds   = Subset(val_view,   val_indices.tolist())\n",
    "\n",
    "    # 4) (Opsiyonel) Sƒ±nƒ±f sƒ±rasƒ± kontrol√º\n",
    "    expected = [\"no_tumor\", \"tumor\"]\n",
    "    assert train_view.classes == expected, f\"Sƒ±nƒ±f sƒ±rasƒ± {train_view.classes} beklenen {expected} deƒüil!\"\n",
    "\n",
    "    # 5) DataLoader'lar (deterministik workers)\n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    pin = (DEVICE.type == \"cuda\")\n",
    "    common = dict(num_workers=NUM_WORKERS, pin_memory=pin, worker_init_fn=seed_worker, persistent_workers=bool(NUM_WORKERS))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  generator=gen, **common)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076ecb9",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model & Kayƒ±p Fonksiyonu & Optimizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7da69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name: str, num_classes: int = 2):\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    ImageFolder ya da Subset(ImageFolder) kabul eder.\n",
    "    \"\"\"\n",
    "    # Subset ise hedef etiketleri indekslerden topla\n",
    "    if isinstance(dataset, Subset):\n",
    "        base = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "        targets = getattr(base, \"targets\", None)\n",
    "        if targets is None:\n",
    "            raise ValueError(\"Temel dataset'te 'targets' bulunamadƒ±.\")\n",
    "        labels = [targets[i] for i in indices]\n",
    "    else:\n",
    "        labels = list(getattr(dataset, \"targets\", []))\n",
    "\n",
    "    n0 = sum(1 for y in labels if y == 0)\n",
    "    n1 = sum(1 for y in labels if y == 1)\n",
    "    total = max(1, n0 + n1)\n",
    "    # Basit ters frekans aƒüƒ±rlƒ±klandƒ±rmasƒ±\n",
    "    w0 = total / (2.0 * max(1, n0))\n",
    "    w1 = total / (2.0 * max(1, n1))\n",
    "    return torch.tensor([w0, w1], dtype=torch.float)\n",
    "    \n",
    "def make_optimizer(model, lr=3e-4, weight_decay=1e-4):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600cff1",
   "metadata": {},
   "source": [
    "## üîÅ Eƒüitim & Doƒürulama D√∂ng√ºs√º (History ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7926b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class History:\n",
    "    train_loss: list\n",
    "    val_loss: list\n",
    "    train_acc: list\n",
    "    val_acc: list\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def run_one_epoch(model, loader, criterion, optimizer=None, scaler: GradScaler = None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler is not None and DEVICE.type == \"cuda\":\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "def train_model_one_run(model_name: str,\n",
    "                        max_epochs: int = MAX_EPOCHS_DEFAULT,\n",
    "                        lr: float = 3e-4,\n",
    "                        weight_decay: float = 1e-4,\n",
    "                        batch_size: int = BATCH_SIZE):\n",
    "    input_size = MODEL_PROFILES[model_name][\"input_size\"]\n",
    "\n",
    "    # √ñNEMLƒ∞: Artƒ±k batch_size parametresini ger√ßekten kullanƒ±yoruz\n",
    "    train_loader, val_loader, test_loader, full_train = make_dataloaders(\n",
    "        input_size=input_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model = build_model(model_name).to(DEVICE)\n",
    "    class_weights = compute_class_weights(full_train).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = make_optimizer(model, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "    history = History(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 7\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            model, train_loader, criterion,\n",
    "            optimizer=optimizer, scaler=scaler\n",
    "        )\n",
    "        va_loss, va_acc = run_one_epoch(\n",
    "            model, val_loader, criterion,\n",
    "            optimizer=None, scaler=None\n",
    "        )\n",
    "\n",
    "        history.train_loss.append(tr_loss)\n",
    "        history.val_loss.append(va_loss)\n",
    "        history.train_acc.append(tr_acc)\n",
    "        history.val_acc.append(va_acc)\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732f969",
   "metadata": {},
   "source": [
    "# üîé Random Search Yardƒ±mcƒ±larƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4cf7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# üîé Random Search Yardƒ±mcƒ±larƒ±\n",
    "# =========================\n",
    "import math, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def sample_from(space: dict, rng: random.Random) -> dict:\n",
    "    \"\"\"\n",
    "    Hyperparam alanƒ±ndan tek bir √∂rnek se√ßer.\n",
    "    Not: rng dƒ±≈üarƒ±dan verilir; global random seed resetlerinden etkilenmez.\n",
    "    \n",
    "    Hyperparam alanƒ±ndan tek bir √∂rnek se√ßer.\n",
    "    space formatƒ± √∂rn:\n",
    "    {\n",
    "        \"lr\": {\"type\": \"loguniform\", \"low\": 1e-5, \"high\": 1e-2},\n",
    "        \"weight_decay\": {\"type\": \"loguniform\", \"low\": 1e-6, \"high\": 1e-3},\n",
    "        \"batch_size\": {\"type\": \"choice\", \"values\": [8, 16, 32]},\n",
    "        \"model_name\": {\"type\": \"choice\", \"values\": [\"efficientnet_v2_m\", \"resnet50\"]}\n",
    "    }\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, cfg in space.items():\n",
    "        t = cfg[\"type\"]\n",
    "        if t == \"choice\":\n",
    "            out[k] = rng.choice(cfg[\"values\"])\n",
    "        elif t == \"uniform\":\n",
    "            lo, hi = float(cfg[\"low\"]), float(cfg[\"high\"])\n",
    "            out[k] = rng.random() * (hi - lo) + lo\n",
    "        elif t == \"loguniform\":\n",
    "            lo, hi = math.log(float(cfg[\"low\"])), math.log(float(cfg[\"high\"]))\n",
    "            out[k] = math.exp(rng.random() * (hi - lo) + lo)\n",
    "        else:\n",
    "            raise ValueError(f\"Bilinmeyen t√ºr: {t}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def try_set_seed(seed: int):\n",
    "    try:\n",
    "        set_seed(seed)  # Notebook'ta varsa kullan\n",
    "    except Exception:\n",
    "        # Yoksa sessizce ge√ß\n",
    "        pass\n",
    "\n",
    "def run_one_trial(cfg,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  model_name,\n",
    "                  metric=\"val_acc\",\n",
    "                  seed=42):\n",
    "    try_set_seed(seed)\n",
    "\n",
    "    model, history, test_loader = train_model_one_run(\n",
    "        model_name=model_name,\n",
    "        max_epochs=max_epochs,\n",
    "        lr=cfg[\"lr\"],\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "        batch_size=cfg[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    trial_name = (\n",
    "        f\"{model_name}_lr{cfg['lr']:.2e}\"\n",
    "        f\"_wd{cfg['weight_decay']:.2e}\"\n",
    "        f\"_bs{cfg['batch_size']}\"\n",
    "    )\n",
    "    trial_dir = output_root / trial_name\n",
    "    ensure_dir(trial_dir)\n",
    "\n",
    "    try:\n",
    "        plot_and_save_history(history, trial_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Plot failed:\", e)\n",
    "\n",
    "    # Metric se√ßimi\n",
    "    if metric == \"val_loss\" and history.val_loss:\n",
    "        metric_value = min(history.val_loss)\n",
    "    elif metric == \"val_acc\" and history.val_acc:\n",
    "        metric_value = max(history.val_acc)\n",
    "    else:\n",
    "        print(f\"[WARN] Metric {metric} desteklenmiyor, val_acc kullanƒ±lacak.\")\n",
    "        metric_value = max(history.val_acc) if history.val_acc else float(\"-inf\")\n",
    "\n",
    "    return metric_value, trial_dir\n",
    "\n",
    "\n",
    "def random_search(space,\n",
    "                  n_trials,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  metric=\"val_acc\",\n",
    "                  greater_is_better=True,\n",
    "                  model_name=None):\n",
    "    \"\"\"\n",
    "    Tek bir model (model_name) i√ßin random search yapar.\n",
    "    space: lr, weight_decay, batch_size gibi hiperparametre aralƒ±ƒüƒ±\n",
    "    \"\"\"\n",
    "    ensure_dir(output_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    results = []\n",
    "    sampler_rng = random.Random(12345)  # sadece hyperparam √∂rneklemek i√ßin ayrƒ± RNG\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # Hiperparametreleri random se√ß\n",
    "        cfg = sample_from(space, sampler_rng)\n",
    "\n",
    "        # Bir trial ko≈ü\n",
    "        metric_value, trial_dir = run_one_trial(\n",
    "            cfg=cfg,\n",
    "            max_epochs=max_epochs,\n",
    "            output_root=output_root,\n",
    "            model_name=model_name,\n",
    "            metric=metric,\n",
    "            seed=42 + i\n",
    "        )\n",
    "\n",
    "        # Sonucu kaydet\n",
    "        row = {\n",
    "            **cfg,\n",
    "            \"metric_name\": metric,\n",
    "            \"metric_value\": metric_value,\n",
    "            \"trial_dir\": str(trial_dir),\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "    # En iyi sonucu se√ß\n",
    "    if greater_is_better:\n",
    "        best_row = max(results, key=lambda r: r[\"metric_value\"])\n",
    "    else:\n",
    "        best_row = min(results, key=lambda r: r[\"metric_value\"])\n",
    "\n",
    "    best_cfg = {k: best_row[k] for k in space.keys()}\n",
    "\n",
    "    # Pandas DataFrame'e √ßevir\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV olarak kaydet\n",
    "    csv_path = output_root / \"random_search_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Random search sonu√ßlarƒ± kaydedildi:\", csv_path)\n",
    "\n",
    "    return best_cfg, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98561d",
   "metadata": {},
   "source": [
    "# üöÄ Random Search'i √áalƒ±≈ütƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35619748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c5062fd234d5e9c70fdb5ebd4c844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/95.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search sonu√ßlarƒ± kaydedildi: sweeps/random_search/random_search_results.csv\n",
      "\n",
      "Se√ßilen model: inception_v3\n",
      "En iyi konfig√ºrasyon:\n",
      "{'lr': 0.00010765096851001565, 'weight_decay': 1.072772194472807e-06, 'batch_size': 24}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>trial_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr1.08e-04_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr2.33e-05_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr2.07e-04_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr6.93e-05_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr1.02e-05_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.998018</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr3.28e-04_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.996036</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr2.52e-04_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>32</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.996036</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr1.18e-03_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>24</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.993062</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr1.31e-03_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>16</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>0.990089</td>\n",
       "      <td>sweeps/random_search/inception_v3_lr1.46e-03_w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  weight_decay  batch_size metric_name  metric_value  \\\n",
       "0  0.000108      0.000001          24     val_acc      1.000000   \n",
       "6  0.000023      0.000144          16     val_acc      1.000000   \n",
       "5  0.000207      0.000161          16     val_acc      0.999009   \n",
       "8  0.000069      0.000799          32     val_acc      0.999009   \n",
       "9  0.000010      0.000662          16     val_acc      0.999009   \n",
       "4  0.000328      0.000004           8     val_acc      0.998018   \n",
       "2  0.000252      0.000003           8     val_acc      0.996036   \n",
       "7  0.001184      0.000009          32     val_acc      0.996036   \n",
       "1  0.001313      0.000624          24     val_acc      0.993062   \n",
       "3  0.001464      0.000006          16     val_acc      0.990089   \n",
       "\n",
       "                                           trial_dir  \n",
       "0  sweeps/random_search/inception_v3_lr1.08e-04_w...  \n",
       "6  sweeps/random_search/inception_v3_lr2.33e-05_w...  \n",
       "5  sweeps/random_search/inception_v3_lr2.07e-04_w...  \n",
       "8  sweeps/random_search/inception_v3_lr6.93e-05_w...  \n",
       "9  sweeps/random_search/inception_v3_lr1.02e-05_w...  \n",
       "4  sweeps/random_search/inception_v3_lr3.28e-04_w...  \n",
       "2  sweeps/random_search/inception_v3_lr2.52e-04_w...  \n",
       "7  sweeps/random_search/inception_v3_lr1.18e-03_w...  \n",
       "1  sweeps/random_search/inception_v3_lr1.31e-03_w...  \n",
       "3  sweeps/random_search/inception_v3_lr1.46e-03_w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# üöÄ Random Search'i √áalƒ±≈ütƒ±r\n",
    "# =========================\n",
    "\n",
    "# 1) Arama alanƒ±nƒ± tanƒ±mla (artƒ±k model_name yok, sadece hiperparametreler var)\n",
    "search_space = {\n",
    "    \"lr\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-5,\n",
    "        \"high\": 3e-3,\n",
    "    },\n",
    "    \"weight_decay\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-6,\n",
    "        \"high\": 1e-3,\n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [8, 16, 24, 32],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 2) S√ºp√ºrme ayarlarƒ±\n",
    "N_TRIALS   = 10          # Ka√ß deneme yapƒ±lacak\n",
    "MAX_EPOCHS = 8           # Her denemenin epoch sayƒ±sƒ±\n",
    "METRIC     = \"val_acc\"   # Artƒ±k AUC deƒüil, validation accuracy kullanƒ±yoruz\n",
    "HIGHER_BETTER = True     # val_acc i√ßin b√ºy√ºk olan daha iyidir\n",
    "OUTPUT_ROOT = \"./sweeps/random_search\"\n",
    "\n",
    "# 3) √áalƒ±≈ütƒ±r\n",
    "best, df = random_search(\n",
    "    space=search_space,\n",
    "    n_trials=N_TRIALS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    metric=METRIC,\n",
    "    greater_is_better=HIGHER_BETTER,\n",
    "    model_name=TARGET_MODEL,   # >>> BURASI √ñNEMLƒ∞: sadece se√ßtiƒüin modeli geziyoruz\n",
    ")\n",
    "\n",
    "print(\"\\nSe√ßilen model:\", TARGET_MODEL)\n",
    "print(\"En iyi konfig√ºrasyon:\")\n",
    "print(best)\n",
    "\n",
    "# DataFrame'i g√∂r√ºnt√ºle\n",
    "try:\n",
    "    import IPython\n",
    "    from IPython.display import display\n",
    "    display(df.sort_values(\"metric_value\", ascending=not HIGHER_BETTER))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e821e8",
   "metadata": {},
   "source": [
    "## üìä Deƒüerlendirme + Grafik ve Tablo Kaydƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5103595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def plot_and_save_history(hist, out_dir: str):\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_acc)+1), hist.train_acc, label=\"train_acc\")\n",
    "    plt.plot(range(1, len(hist.val_acc)+1),   hist.val_acc,   label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"accuracy.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_loss)+1), hist.train_loss, label=\"train_loss\")\n",
    "    plt.plot(range(1, len(hist.val_loss)+1),   hist.val_loss,   label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_save(model, test_loader, out_dir: str, model_name: str):\n",
    "    model.eval()\n",
    "    y_true, y_prob, y_pred = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            logits = model(images)\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # pozitif sƒ±nƒ±f \"tumor\"\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "\n",
    "            y_true.extend(labels.numpy().tolist())\n",
    "            y_prob.extend(probs.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # === Hata Matrisi ===\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, CLASS_NAMES, rotation=45)\n",
    "    plt.yticks(tick_marks, CLASS_NAMES)\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_matrix.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # === ROC Eƒürisi ===\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"roc_curve.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # === Metrikler ===\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    recall    = recall_score(y_true, y_pred,    pos_label=1, zero_division=0)  # sensitivity\n",
    "    f1        = f1_score(y_true, y_pred,        pos_label=1, zero_division=0)\n",
    "    kappa     = cohen_kappa_score(y_true, y_pred)\n",
    "    acc       = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Tabloyu PNG olarak kaydet\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    cell_text = [[f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{kappa:.4f}\", f\"{acc:.4f}\"]]\n",
    "    col_labels = [\"Precision\", \"Sensitivity (Recall)\", \"F1-Score\", \"Cohen's Kappa\", \"Accuracy\"]\n",
    "    the_table = ax.table(cellText=cell_text, colLabels=col_labels, loc='center')\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(11)\n",
    "    the_table.scale(1.2, 1.6)\n",
    "    plt.title(\"Deƒüerlendirme Metrikleri\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, \"metrics_table.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Ayrƒ±ca CSV ve classification_report da kaydedelim\n",
    "    import csv\n",
    "    with open(os.path.join(out_dir, \"metrics.csv\"), \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"metric\", \"value\"])\n",
    "        writer.writerow([\"precision\", precision])\n",
    "        writer.writerow([\"recall_sensitivity\", recall])\n",
    "        writer.writerow([\"f1\", f1])\n",
    "        writer.writerow([\"kappa\", kappa])\n",
    "        writer.writerow([\"auc\", roc_auc])\n",
    "        writer.writerow([\"accuracy\", acc])\n",
    "\n",
    "    with open(os.path.join(out_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "        f.write(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall_sensitivity\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": kappa,\n",
    "        \"auc\": roc_auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cce677",
   "metadata": {},
   "source": [
    "# üèÅ En iyi konfig√ºrasyonla yeniden-eƒüitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebe00868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search en iyi hiperparametreler: {'lr': 0.00010765096851001565, 'weight_decay': 1.072772194472807e-06, 'batch_size': 24}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "\n",
      "Final eƒüitim tamamlandƒ±.\n",
      "Metrikler: {'precision': 1.0, 'recall_sensitivity': 0.5776435045317221, 'f1': 0.7322864802757565, 'kappa': 0.5733248542986473, 'auc': 0.9842268848144772, 'accuracy': 0.7851214263756533, 'confusion_matrix': [[1598, 0], [699, 956]]}\n",
      "Sonu√ß klas√∂r√º: results/densenet121_final\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# üèÅ En iyi konfig√ºrasyonla yeniden eƒüitim\n",
    "# =========================\n",
    "\n",
    "# Random Search sonucunda d√∂nen en iyi hiperparametreler 'best' i√ßinde.\n",
    "# (Random Search h√ºcrende: best, df = random_search(...) diye aldƒ±n.)\n",
    "\n",
    "print(\"Random Search en iyi hiperparametreler:\", best)\n",
    "\n",
    "FINAL_EPOCHS = 50  # ƒ∞stersen artƒ±r\n",
    "\n",
    "# Reprod√ºksiyon i√ßin seed\n",
    "try_set_seed(42)\n",
    "\n",
    "# Se√ßtiƒüin model ismi: MODEL_PROFILES h√ºcresinde tanƒ±mlƒ± olmalƒ±\n",
    "# √ñrn: TARGET_MODEL = \"efficientnet_v2_m\"\n",
    "\n",
    "model, history, test_loader = train_model_one_run(\n",
    "    model_name=TARGET_MODEL,\n",
    "    max_epochs=FINAL_EPOCHS,\n",
    "    lr=best[\"lr\"],\n",
    "    weight_decay=best[\"weight_decay\"],\n",
    "    batch_size=best[\"batch_size\"],\n",
    ")\n",
    "\n",
    "# √áƒ±ktƒ±larƒ± kaydet\n",
    "final_out_dir = os.path.join(\"results\", f\"{TARGET_MODEL}_final\")\n",
    "os.makedirs(final_out_dir, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(final_out_dir, \"best.pt\"))\n",
    "plot_and_save_history(history, final_out_dir)\n",
    "\n",
    "summary = evaluate_and_save(model, test_loader, final_out_dir, TARGET_MODEL)\n",
    "\n",
    "print(\"\\nFinal eƒüitim tamamlandƒ±.\")\n",
    "print(\"Metrikler:\", summary)\n",
    "print(\"Sonu√ß klas√∂r√º:\", final_out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c593d",
   "metadata": {},
   "source": [
    "## üöÄ Manuel Eƒüit ve T√ºm √áƒ±ktƒ±larƒ± Kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20860717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se√ßilen model: inception_v3 (input_size=299)\n",
      "√áƒ±ktƒ±lar: results/inception_v3_20251227_105438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n",
      "\n",
      "√ñzet:\n",
      "- accuracy.png\n",
      "- loss.png\n",
      "- confusion_matrix.png\n",
      "- roc_curve.png\n",
      "- metrics_table.png\n",
      "- metrics.csv\n",
      "- classification_report.txt\n",
      "- best.pt\n",
      "\n",
      "Metrikler: {'precision': 1.0, 'recall_sensitivity': 0.43021148036253776, 'f1': 0.6016054076890579, 'kappa': 0.4258836251628493, 'auc': 0.9005679304568777, 'accuracy': 0.7101137411620043, 'confusion_matrix': [[1598, 0], [943, 712]]}\n"
     ]
    }
   ],
   "source": [
    "# Hiperparametreleri burada deƒüi≈ütirin\n",
    "LR = 0.00010765096851001565\n",
    "WEIGHT_DECAY = 1.072772194472807e-06\n",
    "MAX_EPOCHS   = MAX_EPOCHS_DEFAULT\n",
    "\n",
    "run_tag = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = os.path.join(\"results\", f\"{SELECT_ONE_MODEL}_{run_tag}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Se√ßilen model: {SELECT_ONE_MODEL} (input_size={INPUT_SIZE})\")\n",
    "print(\"√áƒ±ktƒ±lar:\", out_dir)\n",
    "\n",
    "model, hist, test_loader = train_model_one_run(\n",
    "    SELECT_ONE_MODEL,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=24               # Random search h√ºcresinden sonra baƒülantƒ± koparsa eƒüitimi buradan devam et ama buradaki batch size deƒüi≈ütir.\n",
    ")\n",
    "\n",
    "# En iyi aƒüƒ±rlƒ±klarƒ± kaydet\n",
    "torch.save(model.state_dict(), os.path.join(out_dir, \"best.pt\"))\n",
    "\n",
    "# Eƒüitim ge√ßmi≈üi g√∂rselleri\n",
    "plot_and_save_history(hist, out_dir)\n",
    "\n",
    "# Test set deƒüerlendirme ve g√∂rseller\n",
    "summary = evaluate_and_save(model, test_loader, out_dir, SELECT_ONE_MODEL)\n",
    "\n",
    "print(\"\"\"\n",
    "√ñzet:\n",
    "- accuracy.png\n",
    "- loss.png\n",
    "- confusion_matrix.png\n",
    "- roc_curve.png\n",
    "- metrics_table.png\n",
    "- metrics.csv\n",
    "- classification_report.txt\n",
    "- best.pt\n",
    "\"\"\")\n",
    "print(\"Metrikler:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dda7ad",
   "metadata": {},
   "source": [
    "# Farklƒ± bir veri seti ile test et."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Ayarlar (kendine g√∂re deƒüi≈ütir)\n",
    "# =========================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_NAME   = \"tf_efficientnetv2_m\"                      # Eƒüittiƒüin modelin ismi\n",
    "INPUT_SIZE   = 384                             # ResNet34 i√ßin senin kullandƒ±ƒüƒ±n input_size\n",
    "CLASS_NAMES  = [\"no_tumor\", \"tumor\"]           # Klas√∂r sƒ±rasƒ± ile aynƒ± olmalƒ±\n",
    "\n",
    "WEIGHTS_PATH = \"best.pt\"  # Eƒüittiƒüin best.pt'nin yolu\n",
    "EXTERNAL_ROOT = \"Yeni_Test\"                   # ExternalTest/no_tumor , ExternalTest/tumor\n",
    "\n",
    "OUT_DIR = os.path.join(\"results\", f\"{MODEL_NAME}_external_only\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Model:\", MODEL_NAME)\n",
    "print(\"Aƒüƒ±rlƒ±k dosyasƒ±:\", WEIGHTS_PATH)\n",
    "print(\"External test k√∂k klas√∂r√º:\", EXTERNAL_ROOT)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Eval transform (eƒüitimde kullandƒ±ƒüƒ±nla aynƒ± olmalƒ±)\n",
    "# Eƒüer notebook'ta zaten build_transforms tanƒ±mlƒ±ysa onu kullanƒ±yoruz.\n",
    "# =========================\n",
    "try:\n",
    "    # Senin build_transforms fonksiyonun: (train_tfms, eval_tfms) d√∂nd√ºr√ºyordu\n",
    "    _, eval_tfms = build_transforms(INPUT_SIZE)\n",
    "except NameError:\n",
    "    # build_transforms yoksa, SquarePad + Resize + Normalize(0.5) ile yeniden tanƒ±mla\n",
    "    from torchvision import transforms\n",
    "\n",
    "    class SquarePad:\n",
    "        def __call__(self, img):\n",
    "            w, h = img.size\n",
    "            if w == h:\n",
    "                return img\n",
    "            size = max(w, h)\n",
    "            pad_left   = (size - w) // 2\n",
    "            pad_top    = (size - h) // 2\n",
    "            pad_right  = size - w - pad_left\n",
    "            pad_bottom = size - h - pad_top\n",
    "            return transforms.functional.pad(\n",
    "                img,\n",
    "                (pad_left, pad_top, pad_right, pad_bottom),\n",
    "                fill=0\n",
    "            )\n",
    "\n",
    "    eval_tfms = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5],\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# External test DataLoader\n",
    "# =========================\n",
    "external_dataset = datasets.ImageFolder(EXTERNAL_ROOT, transform=eval_tfms)\n",
    "external_loader  = DataLoader(\n",
    "    external_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"ExternalTest sƒ±nƒ±flarƒ±:\", external_dataset.classes)\n",
    "assert external_dataset.classes == CLASS_NAMES, (\n",
    "    f\"ExternalTest klas√∂r sƒ±rasƒ± {external_dataset.classes}, \"\n",
    "    f\"CLASS_NAMES {CLASS_NAMES} ile aynƒ± olmalƒ±.\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model kurulumu ve aƒüƒ±rlƒ±k y√ºkleme\n",
    "# =========================\n",
    "def build_model(model_name: str, num_classes: int = 2):\n",
    "    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model(MODEL_NAME, num_classes=len(CLASS_NAMES))\n",
    "\n",
    "state = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ve aƒüƒ±rlƒ±klar y√ºklendi.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# External test √ºzerinde deƒüerlendirme + g√∂rselle≈ütirme\n",
    "# evaluate_and_save fonksiyonu daha √∂nce notebook'ta tanƒ±mlƒ± olmalƒ±.\n",
    "# =========================\n",
    "external_summary = evaluate_and_save(\n",
    "    model=model,\n",
    "    test_loader=external_loader,\n",
    "    out_dir=OUT_DIR,\n",
    "    model_name=MODEL_NAME + \"_external\",\n",
    ")\n",
    "\n",
    "print(\"\\nDƒ±≈ü test (ExternalTest) metrikleri:\")\n",
    "print(external_summary)\n",
    "print(\"T√ºm g√∂rseller ve metrikler burada kaydedildi:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268eb26e",
   "metadata": {},
   "source": [
    "# windows'ta √ßalƒ±≈ütƒ±rarak farklƒ± bir veri seti ile test etme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db149791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Ayarlar (kendine g√∂re deƒüi≈ütir)\n",
    "# =========================\n",
    "MODEL_NAME   = \"tf_efficientnetv2_m\"                      # Eƒüittiƒüin modelin ismi\n",
    "INPUT_SIZE   = 384                                       # ResNet34 i√ßin kullandƒ±ƒüƒ±n input_size\n",
    "CLASS_NAMES  = [\"no_tumor\", \"tumor\"]           # Klas√∂r sƒ±rasƒ± ile aynƒ± olmalƒ±\n",
    "WEIGHTS_PATH = \"best.pt\"  # Eƒüittiƒüin best.pt'nin yolu\n",
    "EXTERNAL_ROOT = \"Yeni_Test\"                   # ExternalTest/no_tumor , ExternalTest/tumor\n",
    "\n",
    "OUT_DIR = os.path.join(\"results\", f\"{MODEL_NAME}_external_only\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Model:\", MODEL_NAME)\n",
    "print(\"Aƒüƒ±rlƒ±k dosyasƒ±:\", WEIGHTS_PATH)\n",
    "print(\"External test k√∂k klas√∂r√º:\", EXTERNAL_ROOT)\n",
    "\n",
    "# =========================\n",
    "# Eval transform (eƒüitimde kullandƒ±ƒüƒ±nla aynƒ± olmalƒ±)\n",
    "# build_transforms varsa onu kullan, yoksa fallback\n",
    "# =========================\n",
    "try:\n",
    "    # Senin notebook'undaki fonksiyon: (train_tfms, eval_tfms) d√∂nd√ºr√ºyordu\n",
    "    _, eval_tfms = build_transforms(INPUT_SIZE)\n",
    "except NameError:\n",
    "    from torchvision import transforms\n",
    "\n",
    "    class SquarePad:\n",
    "        def __call__(self, img):\n",
    "            w, h = img.size\n",
    "            if w == h:\n",
    "                return img\n",
    "            size = max(w, h)\n",
    "            pad_left   = (size - w) // 2\n",
    "            pad_top    = (size - h) // 2\n",
    "            pad_right  = size - w - pad_left\n",
    "            pad_bottom = size - h - pad_top\n",
    "            return transforms.functional.pad(\n",
    "                img,\n",
    "                (pad_left, pad_top, pad_right, pad_bottom),\n",
    "                fill=0\n",
    "            )\n",
    "\n",
    "    eval_tfms = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5],\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# External test DataLoader  (num_workers = 0 !)\n",
    "# =========================\n",
    "external_dataset = datasets.ImageFolder(EXTERNAL_ROOT, transform=eval_tfms)\n",
    "external_loader  = DataLoader(\n",
    "    external_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,      # <-- Windows + notebook i√ßin g√ºvenli\n",
    "    pin_memory=False,   # GPU varsa bile zorunlu deƒüil\n",
    ")\n",
    "\n",
    "print(\"ExternalTest sƒ±nƒ±flarƒ±:\", external_dataset.classes)\n",
    "assert external_dataset.classes == CLASS_NAMES, (\n",
    "    f\"ExternalTest klas√∂r sƒ±rasƒ± {external_dataset.classes}, \"\n",
    "    f\"CLASS_NAMES {CLASS_NAMES} ile aynƒ± olmalƒ±.\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model kurulumu ve aƒüƒ±rlƒ±k y√ºkleme\n",
    "# =========================\n",
    "def build_model(model_name: str, num_classes: int = 2):\n",
    "    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model(MODEL_NAME, num_classes=len(CLASS_NAMES))\n",
    "\n",
    "state = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ve aƒüƒ±rlƒ±klar y√ºklendi.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# External test √ºzerinde deƒüerlendirme + g√∂rselle≈ütirme\n",
    "# (evaluate_and_save fonksiyonun yukarƒ±da tanƒ±mlƒ± olmalƒ±)\n",
    "# =========================\n",
    "external_summary = evaluate_and_save(\n",
    "    model=model,\n",
    "    test_loader=external_loader,\n",
    "    out_dir=OUT_DIR,\n",
    "    model_name=MODEL_NAME + \"_external\",\n",
    ")\n",
    "\n",
    "print(\"\\nDƒ±≈ü test (ExternalTest) metrikleri:\")\n",
    "print(external_summary)\n",
    "print(\"T√ºm g√∂rseller ve metrikler burada kaydedildi:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
