{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ad5794",
   "metadata": {},
   "source": [
    "# ğŸ§  Brain MRI Ä°kili SÄ±nÄ±flandÄ±rma â€” Modeli Tek Tek SeÃ§erek EÄŸitim\n",
    "\n",
    "\n",
    "Bu defter, **sadece ikili sÄ±nÄ±flandÄ±rma** (labels: **`tumor`** ve **`no_tumor`**) iÃ§in dÃ¼zenlendi.\n",
    "AÅŸaÄŸÄ±daki hÃ¼creler ile **modeli tek tek kendiniz seÃ§ip** eÄŸitebilir; her eÄŸitim iÃ§in **ayrÄ± ayrÄ±**:\n",
    "- Accuracy grafiÄŸi,\n",
    "- Loss grafiÄŸi,\n",
    "- **Hata matrisi (Confusion Matrix)**,\n",
    "- **ROC eÄŸrisi (AUC ile)**,\n",
    "- ve **tablo halinde Sensitivity (Recall), Precision, F1, Cohenâ€™s Kappa**\n",
    "\n",
    "oluÅŸturup **PNG olarak kaydedebilirsiniz**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441cfc8",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Kurulumlar ve KÃ¼tÃ¼phaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e670597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install -q timm scikit-learn torchmetrics\n",
    "\n",
    "import os, math, time, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm  # Ã§ok sayÄ±da SOTA mimari\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ReprodÃ¼ksiyon\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# DataLoader iÅŸÃ§ileri iÃ§in deterministik davranÄ±ÅŸ\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# (Opsiyonel) PyTorch 2.xâ€™te matmul hesaplarÄ±nÄ± â€œhighâ€ hassasiyete Ã§ekerek olasÄ± hÄ±z optimizasyonu deniyor; desteklenmezse sessizce geÃ§iyor.\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 1) # DataLoader iÃ§in en fazla 4 olmak Ã¼zere CPU Ã§ekirdek sayÄ±sÄ± kadar iÅŸÃ§i belirliyor.\n",
    "BATCH_SIZE = 32\n",
    "VAL_RATIO = 0.1   # verinin %10â€™u validasyon iÃ§in ayrÄ±lacak.\n",
    "MAX_EPOCHS_DEFAULT = 50 \n",
    "AUG_STRENGTH = 0.3  # 0.0 => aug kapalÄ±, 0.3 => agresif (train tarafÄ±nda)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb666295",
   "metadata": {},
   "source": [
    "## ğŸ§­ Yollar ve Temel Ayarlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d029f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ”§ DÄ°ZÄ°N YAPISI\n",
    "# DATA_ROOT:\n",
    "#   â”œâ”€â”€ Training\n",
    "#   â”‚    â”œâ”€â”€ tumor\n",
    "#   â”‚    â””â”€â”€ no_tumor\n",
    "#   â””â”€â”€ Testing\n",
    "#        â”œâ”€â”€ tumor\n",
    "#        â””â”€â”€ no_tumor\n",
    "\n",
    "DATA_ROOT = \"Dataset\"  # Veri seti kÃ¶k dizin.\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"Training\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"Testing\")\n",
    "\n",
    "# Etiket sÄ±rasÄ± sabitliyorum: 0 = no_tumor, 1 = tumor\n",
    "CLASS_NAMES = [\"no_tumor\", \"tumor\"]\n",
    "POS_LABEL = 1  # ROC/metrics iÃ§in pozitif sÄ±nÄ±f (tumor)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True) # Ã‡Ä±ktÄ±lar iÃ§in results klasÃ¶rÃ¼ yoksa oluÅŸturuluyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadcc60",
   "metadata": {},
   "source": [
    "## ğŸ§© Model Profilleri (Input Size) ve Tekli SeÃ§im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709ae705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: hybrid_dn121_effb0 Input: 256\n"
     ]
    }
   ],
   "source": [
    "# GiriÅŸ boyutu Ã¶nerileri\n",
    "MODEL_PROFILES = {\n",
    "    \"resnet34\": {\"input_size\": 224},\n",
    "    \"resnet50\": {\"input_size\": 224},\n",
    "    \"densenet121\": {\"input_size\": 224},\n",
    "    \"inception_v3\": {\"input_size\": 299},\n",
    "    \"efficientnet_b0\": {\"input_size\": 224},\n",
    "    \"mobilenetv2_100\": {\"input_size\": 224},\n",
    "    \"convnext_tiny\": {\"input_size\": 224},\n",
    "\n",
    "    # âœ… HYBRID\n",
    "    \"hybrid_dn121_effb0\": {\"input_size\": 256},\n",
    "}\n",
    "\n",
    "ALL_MODELS = list(MODEL_PROFILES.keys())\n",
    "\n",
    "# âœ… Senin kararÄ±n: dataset karÄ±ÅŸÄ±k olduÄŸu iÃ§in her ÅŸeyi 256'ya sabitle\n",
    "OVERRIDE_INPUT_SIZE = 256\n",
    "\n",
    "# Buradan TEK bir modeli seÃ§in\n",
    "SELECT_ONE_MODEL = \"hybrid_dn121_effb0\"  # <- hibrit modeli seÃ§tik\n",
    "TARGET_MODEL = SELECT_ONE_MODEL\n",
    "\n",
    "INPUT_SIZE = OVERRIDE_INPUT_SIZE if OVERRIDE_INPUT_SIZE is not None else MODEL_PROFILES[SELECT_ONE_MODEL][\"input_size\"]\n",
    "print(\"Selected:\", SELECT_ONE_MODEL, \"Input:\", INPUT_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ecac0",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ DÃ¶nÃ¼ÅŸÃ¼mler (Pad + Resize + Normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2710f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "\n",
    "# Model bazlÄ± mean/std cache\n",
    "_MEAN_STD_CACHE = {}\n",
    "\n",
    "# âœ… Hibrit iÃ§in mean/std hangi backbone'dan alÄ±nacak?\n",
    "_HYBRID_MEANSTD_SOURCE = {\n",
    "    \"hybrid_dn121_effb0\": \"densenet121\"  # ikisi de ImageNet normalize kullandÄ±ÄŸÄ± iÃ§in fark etmez\n",
    "}\n",
    "\n",
    "def get_mean_std_from_timm(model_name: str):\n",
    "    \"\"\"\n",
    "    timm modelinin default_cfg/pretrained_cfg iÃ§inden mean/std Ã§eker.\n",
    "    Hibrit model adÄ±nÄ± desteklemek iÃ§in backbone'a map edilir.\n",
    "    \"\"\"\n",
    "    name = _HYBRID_MEANSTD_SOURCE.get(model_name, model_name)\n",
    "\n",
    "    if name in _MEAN_STD_CACHE:\n",
    "        return _MEAN_STD_CACHE[name]\n",
    "\n",
    "    m = timm.create_model(name, pretrained=False, num_classes=2)\n",
    "    cfg = resolve_data_config({}, model=m)\n",
    "\n",
    "    mean, std = cfg[\"mean\"], cfg[\"std\"]\n",
    "    _MEAN_STD_CACHE[name] = (mean, std)\n",
    "    del m\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            return img\n",
    "        size = max(w, h)\n",
    "        pad_left = (size - w) // 2\n",
    "        pad_top = (size - h) // 2\n",
    "        pad_right = size - w - pad_left\n",
    "        pad_bottom = size - h - pad_top\n",
    "        return transforms.functional.pad(\n",
    "            img, (pad_left, pad_top, pad_right, pad_bottom), fill=0\n",
    "        )\n",
    "\n",
    "\n",
    "def build_transforms(model_name: str, input_size: int, aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Transform Ã¼retimi.\n",
    "\n",
    "    aug_strength:\n",
    "      - 0.0  => augmentasyon kapalÄ± (train = eval)\n",
    "      - 0.0 < => train'de augmentasyon aÃ§Ä±k (0.3 = agresif)\n",
    "    Not: aug_strength 0.3 Ã¼stÃ¼ verilirse 0.3'e kÄ±rpÄ±lÄ±r.\n",
    "    \"\"\"\n",
    "    mean, std = get_mean_std_from_timm(model_name)\n",
    "\n",
    "    base = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    # Aug kapalÄ±ysa train = eval (tam istediÄŸin davranÄ±ÅŸ)\n",
    "    try:\n",
    "        a = float(aug_strength)\n",
    "    except Exception:\n",
    "        a = 0.0\n",
    "    a = max(0.0, min(0.3, a))\n",
    "    if a <= 0.0:\n",
    "        return base, base\n",
    "\n",
    "    # 0..1 Ã¶lÃ§eÄŸe taÅŸÄ± (0.3 => 1.0 yoÄŸunluk)\n",
    "    s = a / 0.3\n",
    "\n",
    "    # PIL seviyesinde (ToTensor'dan Ã¶nce) augmentasyonlar\n",
    "    pil_aug = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.50 * s),\n",
    "        transforms.RandomVerticalFlip(p=0.15 * s),\n",
    "        transforms.RandomRotation(degrees=15.0 * s),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0.0,\n",
    "            translate=(0.08 * s, 0.08 * s),\n",
    "            scale=(1.0 - 0.10 * s, 1.0 + 0.10 * s),\n",
    "        ),\n",
    "        transforms.RandomPerspective(distortion_scale=0.20 * s, p=0.25 * s),\n",
    "        transforms.RandomApply(\n",
    "            [transforms.ColorJitter(\n",
    "                brightness=0.40 * s,\n",
    "                contrast=0.40 * s,\n",
    "                saturation=0.25 * s,\n",
    "                hue=0.05 * s,\n",
    "            )],\n",
    "            p=0.70 * s\n",
    "        ),\n",
    "        transforms.RandomApply(\n",
    "            [transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5 * s + 0.1))],\n",
    "            p=0.20 * s\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        SquarePad(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        pil_aug,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "        # Tensor seviyesinde augmentasyon (masking benzeri regularization)\n",
    "        transforms.RandomErasing(\n",
    "            p=0.25 * s,\n",
    "            scale=(0.02, 0.10),\n",
    "            ratio=(0.3, 3.3),\n",
    "            value=\"random\"\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    eval_tfms = base\n",
    "    return train_tfms, eval_tfms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59819689",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Dataset & DataLoader'lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9673c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def make_dataloaders(model_name: str, input_size: int, batch_size: int = BATCH_SIZE, val_ratio: float = VAL_RATIO, aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Train/Val/test DataLoader kurulumunu doÄŸru transform'larla yapar.\n",
    "    - Train: train_tfms\n",
    "    - Val/Test: eval_tfms\n",
    "    AyrÄ±ca train subset'i geri dÃ¶ndÃ¼rÃ¼r (sÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ± iÃ§in).\n",
    "    \"\"\"\n",
    "    train_tfms, eval_tfms = build_transforms(model_name, input_size, aug_strength=aug_strength)\n",
    "\n",
    "    # 1) Stratified split iÃ§in base dataset + etiketler\n",
    "    base_ds = datasets.ImageFolder(TRAIN_DIR)  # transform YOK\n",
    "    y = np.array(base_ds.targets)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)\n",
    "    train_indices, val_indices = next(sss.split(np.zeros(len(y)), y))\n",
    "\n",
    "    # 2) AyrÄ± gÃ¶rÃ¼nÃ¼mler: train vs val/test iÃ§in farklÄ± transform\n",
    "    train_view = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "    val_view   = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
    "    test_ds    = datasets.ImageFolder(TEST_DIR,  transform=eval_tfms)\n",
    "\n",
    "    # 3) Ä°ndeksleri Subset'lere uygula\n",
    "    train_ds = Subset(train_view, train_indices.tolist())\n",
    "    val_ds   = Subset(val_view,   val_indices.tolist())\n",
    "\n",
    "    # 4) (Opsiyonel) SÄ±nÄ±f sÄ±rasÄ± kontrolÃ¼\n",
    "    expected = [\"no_tumor\", \"tumor\"]\n",
    "    assert train_view.classes == expected, f\"SÄ±nÄ±f sÄ±rasÄ± {train_view.classes} beklenen {expected} deÄŸil!\"\n",
    "\n",
    "    # 5) DataLoader'lar (deterministik workers)\n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    pin = (DEVICE.type == \"cuda\")\n",
    "    common = dict(num_workers=NUM_WORKERS, pin_memory=pin, worker_init_fn=seed_worker, persistent_workers=bool(NUM_WORKERS))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  generator=gen, **common)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, generator=gen, **common)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076ecb9",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Model & KayÄ±p Fonksiyonu & Optimizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7da69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDN121_EffB0(nn.Module):\n",
    "    \"\"\"\n",
    "    DenseNet121 + EfficientNetB0 hibrit:\n",
    "    - Ä°ki backbone'dan pooled feature Ã§Ä±karÄ±r (num_classes=0, global_pool='avg')\n",
    "    - Concatenate + kÃ¼Ã§Ã¼k bir MLP head ile 2 sÄ±nÄ±f logits Ã¼retir\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, head_dim=256, dropout=0.2, freeze_backbones=False):\n",
    "        super().__init__()\n",
    "        self.bb1 = timm.create_model(\"densenet121\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.bb2 = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        if freeze_backbones:\n",
    "            for p in self.bb1.parameters(): p.requires_grad = False\n",
    "            for p in self.bb2.parameters(): p.requires_grad = False\n",
    "\n",
    "        dim1 = getattr(self.bb1, \"num_features\")\n",
    "        dim2 = getattr(self.bb2, \"num_features\")\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(dim1 + dim2, head_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_dim, num_classes)   # âœ… logits (softmax yok)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.bb1(x)\n",
    "        f2 = self.bb2(x)\n",
    "        feats = torch.cat([f1, f2], dim=1)\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "def build_model(model_name: str, num_classes: int = 2):\n",
    "    if model_name == \"hybrid_dn121_effb0\":\n",
    "        # freeze_backbones=True dersen overfitting azalÄ±r ama tavan performansÄ± dÃ¼ÅŸebilir\n",
    "        return HybridDN121_EffB0(num_classes=num_classes, head_dim=256, dropout=0.2, freeze_backbones=False)\n",
    "\n",
    "    # normal timm modeller\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    ImageFolder ya da Subset(ImageFolder) kabul eder.\n",
    "    \"\"\"\n",
    "    # Subset ise hedef etiketleri indekslerden topla\n",
    "    if isinstance(dataset, Subset):\n",
    "        base = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "        targets = getattr(base, \"targets\", None)\n",
    "        if targets is None:\n",
    "            raise ValueError(\"Temel dataset'te 'targets' bulunamadÄ±.\")\n",
    "        labels = [targets[i] for i in indices]\n",
    "    else:\n",
    "        labels = list(getattr(dataset, \"targets\", []))\n",
    "\n",
    "    n0 = sum(1 for y in labels if y == 0)\n",
    "    n1 = sum(1 for y in labels if y == 1)\n",
    "    total = max(1, n0 + n1)\n",
    "    # Basit ters frekans aÄŸÄ±rlÄ±klandÄ±rmasÄ±\n",
    "    w0 = total / (2.0 * max(1, n0))\n",
    "    w1 = total / (2.0 * max(1, n1))\n",
    "    return torch.tensor([w0, w1], dtype=torch.float)\n",
    "    \n",
    "def make_optimizer(model, lr=3e-4, weight_decay=1e-4):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600cff1",
   "metadata": {},
   "source": [
    "## ğŸ” EÄŸitim & DoÄŸrulama DÃ¶ngÃ¼sÃ¼ (History ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7926b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class History:\n",
    "    train_loss: list\n",
    "    val_loss: list\n",
    "    train_acc: list\n",
    "    val_acc: list\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def run_one_epoch(model, loader, criterion, optimizer=None, scaler: GradScaler = None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler is not None and DEVICE.type == \"cuda\":\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "def train_model_one_run(model_name: str,\n",
    "                        max_epochs: int = MAX_EPOCHS_DEFAULT,\n",
    "                        lr: float = 3e-4,\n",
    "                        weight_decay: float = 1e-4,\n",
    "                        batch_size: int = BATCH_SIZE,\n",
    "                        aug_strength: float = 0.0):\n",
    "    input_size = MODEL_PROFILES[model_name][\"input_size\"]\n",
    "\n",
    "    # Ã–NEMLÄ°: ArtÄ±k batch_size parametresini gerÃ§ekten kullanÄ±yoruz\n",
    "    train_loader, val_loader, test_loader, full_train = make_dataloaders(\n",
    "        model_name=model_name,\n",
    "        input_size=input_size,\n",
    "        batch_size=batch_size,\n",
    "        aug_strength=aug_strength\n",
    "    )\n",
    "\n",
    "\n",
    "    model = build_model(model_name).to(DEVICE)\n",
    "    class_weights = compute_class_weights(full_train).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = make_optimizer(model, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "    history = History(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 7\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            model, train_loader, criterion,\n",
    "            optimizer=optimizer, scaler=scaler\n",
    "        )\n",
    "        va_loss, va_acc = run_one_epoch(\n",
    "            model, val_loader, criterion,\n",
    "            optimizer=None, scaler=None\n",
    "        )\n",
    "\n",
    "        history.train_loss.append(tr_loss)\n",
    "        history.val_loss.append(va_loss)\n",
    "        history.train_acc.append(tr_acc)\n",
    "        history.val_acc.append(va_acc)\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732f969",
   "metadata": {},
   "source": [
    "# ğŸ” Random Search YardÄ±mcÄ±larÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4cf7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ğŸ” Random Search YardÄ±mcÄ±larÄ±\n",
    "# =========================\n",
    "import math, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def sample_from(space: dict, rng: random.Random) -> dict:\n",
    "    \"\"\"\n",
    "    Hyperparam alanÄ±ndan tek bir Ã¶rnek seÃ§er.\n",
    "    Not: rng dÄ±ÅŸarÄ±dan verilir; global random seed resetlerinden etkilenmez.\n",
    "    \n",
    "    Hyperparam alanÄ±ndan tek bir Ã¶rnek seÃ§er.\n",
    "    space formatÄ± Ã¶rn:\n",
    "    {\n",
    "        \"lr\": {\"type\": \"loguniform\", \"low\": 1e-5, \"high\": 1e-2},\n",
    "        \"weight_decay\": {\"type\": \"loguniform\", \"low\": 1e-6, \"high\": 1e-3},\n",
    "        \"batch_size\": {\"type\": \"choice\", \"values\": [8, 16, 32]},\n",
    "        \"model_name\": {\"type\": \"choice\", \"values\": [\"efficientnet_v2_m\", \"resnet50\"]}\n",
    "    }\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, cfg in space.items():\n",
    "        t = cfg[\"type\"]\n",
    "        if t == \"choice\":\n",
    "            out[k] = rng.choice(cfg[\"values\"])\n",
    "        elif t == \"uniform\":\n",
    "            lo, hi = float(cfg[\"low\"]), float(cfg[\"high\"])\n",
    "            out[k] = rng.random() * (hi - lo) + lo\n",
    "        elif t == \"loguniform\":\n",
    "            lo, hi = math.log(float(cfg[\"low\"])), math.log(float(cfg[\"high\"]))\n",
    "            out[k] = math.exp(rng.random() * (hi - lo) + lo)\n",
    "        else:\n",
    "            raise ValueError(f\"Bilinmeyen tÃ¼r: {t}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def try_set_seed(seed: int):\n",
    "    try:\n",
    "        set_seed(seed)  # Notebook'ta varsa kullan\n",
    "    except Exception:\n",
    "        # Yoksa sessizce geÃ§\n",
    "        pass\n",
    "\n",
    "def run_one_trial(cfg,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  model_name,\n",
    "                  metric=\"val_acc\",\n",
    "                  seed=42,\n",
    "                  aug_strength: float = 0.0):\n",
    "    try_set_seed(seed)\n",
    "\n",
    "    model, history,_, test_loader = train_model_one_run(\n",
    "        model_name=model_name,\n",
    "        max_epochs=max_epochs,\n",
    "        lr=cfg[\"lr\"],\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        aug_strength=aug_strength\n",
    "    )\n",
    "\n",
    "    trial_name = (\n",
    "        f\"{model_name}_lr{cfg['lr']:.2e}\"\n",
    "        f\"_wd{cfg['weight_decay']:.2e}\"\n",
    "        f\"_bs{cfg['batch_size']}\" f\"_aug{aug_strength:.2f}\"\n",
    "    )\n",
    "    trial_dir = output_root / trial_name\n",
    "    ensure_dir(trial_dir)\n",
    "\n",
    "    try:\n",
    "        plot_and_save_history(history, trial_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Plot failed:\", e)\n",
    "\n",
    "    # Metric seÃ§imi\n",
    "    if metric == \"val_loss\" and history.val_loss:\n",
    "        metric_value = min(history.val_loss)\n",
    "    elif metric == \"val_acc\" and history.val_acc:\n",
    "        metric_value = max(history.val_acc)\n",
    "    else:\n",
    "        print(f\"[WARN] Metric {metric} desteklenmiyor, val_acc kullanÄ±lacak.\")\n",
    "        metric_value = max(history.val_acc) if history.val_acc else float(\"-inf\")\n",
    "\n",
    "    return metric_value, trial_dir\n",
    "\n",
    "\n",
    "def random_search(space,\n",
    "                  n_trials,\n",
    "                  max_epochs,\n",
    "                  output_root,\n",
    "                  metric=\"val_acc\",\n",
    "                  greater_is_better=True,\n",
    "                  model_name=None,\n",
    "                  aug_strength: float = 0.0):\n",
    "    \"\"\"\n",
    "    Tek bir model (model_name) iÃ§in random search yapar.\n",
    "    space: lr, weight_decay, batch_size gibi hiperparametre aralÄ±ÄŸÄ±\n",
    "    \"\"\"\n",
    "    ensure_dir(output_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    results = []\n",
    "    sampler_rng = random.Random(12345)  # sadece hyperparam Ã¶rneklemek iÃ§in ayrÄ± RNG\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # Hiperparametreleri random seÃ§\n",
    "        cfg = sample_from(space, sampler_rng)\n",
    "\n",
    "        # Bir trial koÅŸ\n",
    "        metric_value, trial_dir = run_one_trial(\n",
    "            cfg=cfg,\n",
    "            max_epochs=max_epochs,\n",
    "            output_root=output_root,\n",
    "            model_name=model_name,\n",
    "            metric=metric,\n",
    "            seed=42 + i,\n",
    "            aug_strength=aug_strength\n",
    "        )\n",
    "\n",
    "        # Sonucu kaydet\n",
    "        row = {\n",
    "            **cfg,\n",
    "            \"metric_name\": metric,\n",
    "            \"metric_value\": metric_value,\n",
    "            \"trial_dir\": str(trial_dir),\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "    # En iyi sonucu seÃ§\n",
    "    if greater_is_better:\n",
    "        best_row = max(results, key=lambda r: r[\"metric_value\"])\n",
    "    else:\n",
    "        best_row = min(results, key=lambda r: r[\"metric_value\"])\n",
    "\n",
    "    best_cfg = {k: best_row[k] for k in space.keys()}\n",
    "\n",
    "    # Pandas DataFrame'e Ã§evir\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV olarak kaydet\n",
    "    csv_path = output_root / \"random_search_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Random search sonuÃ§larÄ± kaydedildi:\", csv_path)\n",
    "\n",
    "    return best_cfg, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98561d",
   "metadata": {},
   "source": [
    "# ğŸš€ Random Search'i Ã‡alÄ±ÅŸtÄ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35619748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ğŸš€ Random Search'i Ã‡alÄ±ÅŸtÄ±r\n",
    "# =========================\n",
    "\n",
    "# 1) Arama alanÄ±nÄ± tanÄ±mla (artÄ±k model_name yok, sadece hiperparametreler var)\n",
    "search_space = {\n",
    "    \"lr\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-5,\n",
    "        \"high\": 3e-3,\n",
    "    },\n",
    "    \"weight_decay\": {\n",
    "        \"type\": \"loguniform\",\n",
    "        \"low\": 1e-6,\n",
    "        \"high\": 1e-3,\n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [8, 16, 24, 32],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 2) SÃ¼pÃ¼rme ayarlarÄ±\n",
    "N_TRIALS   = 10          # KaÃ§ deneme yapÄ±lacak\n",
    "MAX_EPOCHS = 8           # Her denemenin epoch sayÄ±sÄ±\n",
    "METRIC     = \"val_acc\"   # ArtÄ±k AUC deÄŸil, validation accuracy kullanÄ±yoruz\n",
    "HIGHER_BETTER = True     # val_acc iÃ§in bÃ¼yÃ¼k olan daha iyidir\n",
    "OUTPUT_ROOT = \"./sweeps/random_search\"\n",
    "\n",
    "# 3) Ã‡alÄ±ÅŸtÄ±r\n",
    "best, df = random_search(\n",
    "    space=search_space,\n",
    "    n_trials=N_TRIALS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    metric=METRIC,\n",
    "    greater_is_better=HIGHER_BETTER,\n",
    "    model_name=TARGET_MODEL,   # >>> BURASI Ã–NEMLÄ°: sadece seÃ§tiÄŸin modeli geziyoruz\n",
    "    aug_strength=AUG_STRENGTH,\n",
    ")\n",
    "\n",
    "print(\"\\nSeÃ§ilen model:\", TARGET_MODEL)\n",
    "print(\"En iyi konfigÃ¼rasyon:\")\n",
    "print(best)\n",
    "\n",
    "# DataFrame'i gÃ¶rÃ¼ntÃ¼le\n",
    "try:\n",
    "    import IPython\n",
    "    from IPython.display import display\n",
    "    display(df.sort_values(\"metric_value\", ascending=not HIGHER_BETTER))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e821e8",
   "metadata": {},
   "source": [
    "## ğŸ“Š DeÄŸerlendirme + Grafik ve Tablo KaydÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5103595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def plot_and_save_history(hist, out_dir: str):\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_acc)+1), hist.train_acc, label=\"train_acc\")\n",
    "    plt.plot(range(1, len(hist.val_acc)+1),   hist.val_acc,   label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"accuracy.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(hist.train_loss)+1), hist.train_loss, label=\"train_loss\")\n",
    "    plt.plot(range(1, len(hist.val_loss)+1),   hist.val_loss,   label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_save(model, test_loader, out_dir: str, model_name: str, threshold: float = 0.5):\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_prob, y_pred = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            logits = model(images)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]          # tensor\n",
    "            preds = (probs >= threshold).long()                 # âœ… threshold ile karar\n",
    "\n",
    "            probs = probs.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "\n",
    "            y_true.extend(labels.numpy().tolist())     # istersen alttaki \"gÃ¼venli\" versiyona geÃ§\n",
    "            y_prob.extend(probs.tolist())              # âœ… probs zaten numpy\n",
    "            y_pred.extend(preds.tolist())              # âœ… preds zaten numpy\n",
    "\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # === Hata Matrisi ===\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, CLASS_NAMES, rotation=45)\n",
    "    plt.yticks(tick_marks, CLASS_NAMES)\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_matrix.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # === ROC EÄŸrisi ===\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"roc_curve.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # === Metrikler ===\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    recall    = recall_score(y_true, y_pred,    pos_label=1, zero_division=0)  # sensitivity\n",
    "    f1        = f1_score(y_true, y_pred,        pos_label=1, zero_division=0)\n",
    "    kappa     = cohen_kappa_score(y_true, y_pred)\n",
    "    acc       = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Tabloyu PNG olarak kaydet\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    cell_text = [[f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{kappa:.4f}\", f\"{acc:.4f}\"]]\n",
    "    col_labels = [\"Precision\", \"Sensitivity (Recall)\", \"F1-Score\", \"Cohen's Kappa\", \"Accuracy\"]\n",
    "    the_table = ax.table(cellText=cell_text, colLabels=col_labels, loc='center')\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(11)\n",
    "    the_table.scale(1.2, 1.6)\n",
    "    plt.title(\"DeÄŸerlendirme Metrikleri\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, \"metrics_table.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # AyrÄ±ca CSV ve classification_report da kaydedelim\n",
    "    import csv\n",
    "    with open(os.path.join(out_dir, \"metrics.csv\"), \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"metric\", \"value\"])\n",
    "        writer.writerow([\"precision\", precision])\n",
    "        writer.writerow([\"recall_sensitivity\", recall])\n",
    "        writer.writerow([\"f1\", f1])\n",
    "        writer.writerow([\"kappa\", kappa])\n",
    "        writer.writerow([\"auc\", roc_auc])\n",
    "        writer.writerow([\"accuracy\", acc])\n",
    "\n",
    "    with open(os.path.join(out_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "        f.write(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall_sensitivity\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": kappa,\n",
    "        \"auc\": roc_auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7a7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        logits = model(images)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # tumor olasÄ±lÄ±ÄŸÄ±\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_prob.append(probs.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(y_true), np.concatenate(y_prob)\n",
    "\n",
    "def select_threshold_on_val(y_true, y_prob, objective=\"f1\"):\n",
    "    thresholds = np.linspace(0.0, 1.0, 1001)\n",
    "    best_t, best_score = 0.5, -1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "\n",
    "        if objective == \"f1\":\n",
    "            score = f1_score(y_true, y_pred, pos_label=1)\n",
    "        elif objective == \"bal\":\n",
    "            score = balanced_accuracy_score(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"objective must be 'f1' or 'bal'\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_t = float(t)\n",
    "\n",
    "    return best_t, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cce677",
   "metadata": {},
   "source": [
    "# ğŸ En iyi konfigÃ¼rasyonla yeniden-eÄŸitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe00868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search en iyi hiperparametreler:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_339183/1300154623.py:65: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/tmp/ipykernel_339183/1300154623.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "VAL'dan seÃ§ilen en iyi threshold: 0.023 obj_score: 1.0\n",
      "\n",
      "Final eÄŸitim tamamlandÄ±.\n",
      "Metrikler: {'precision': 1.0, 'recall_sensitivity': 0.7262839879154078, 'f1': 0.8414420721036052, 'kappa': 0.7227557339788279, 'auc': 0.9665261334976879, 'accuracy': 0.8607439286812173, 'confusion_matrix': [[1598, 0], [453, 1202]]}\n",
      "SonuÃ§ klasÃ¶rÃ¼: results/hybrid_dn121_effb0_final\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Search en iyi hiperparametreler:\")\n",
    "\n",
    "FINAL_EPOCHS = 50  # Ä°stersen artÄ±r\n",
    "\n",
    "# ReprodÃ¼ksiyon iÃ§in seed\n",
    "try_set_seed(42)\n",
    "\n",
    "# EÄŸitim\n",
    "model, history, val_loader, test_loader = train_model_one_run(\n",
    "    model_name=TARGET_MODEL,\n",
    "    max_epochs=FINAL_EPOCHS,\n",
    "    lr=6.934684230776975e-05,\n",
    "    weight_decay=0.000798817143317339,\n",
    "    batch_size=32,\n",
    "    aug_strength=AUG_STRENGTH,\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±larÄ± kaydet\n",
    "final_out_dir = os.path.join(\"results\", f\"{TARGET_MODEL}_final\")\n",
    "os.makedirs(final_out_dir, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(final_out_dir, \"best.pt\"))\n",
    "plot_and_save_history(history, final_out_dir)\n",
    "\n",
    "# âœ… 1) VAL'dan threshold seÃ§ (test'e dokunma)\n",
    "y_val, p_val = collect_probs(model, val_loader, DEVICE)\n",
    "best_thr, best_obj = select_threshold_on_val(y_val, p_val, objective=\"f1\")\n",
    "print(\"VAL'dan seÃ§ilen en iyi threshold:\", best_thr, \"obj_score:\", best_obj)\n",
    "\n",
    "with open(os.path.join(final_out_dir, \"best_threshold.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"best_thr={best_thr}\\nobjective=f1\\nobj_score={best_obj}\\n\")\n",
    "\n",
    "# âœ… 2) TEST'i bu threshold ile deÄŸerlendir\n",
    "summary = evaluate_and_save(model, test_loader, final_out_dir, TARGET_MODEL, threshold=best_thr)\n",
    "\n",
    "print(\"\\nFinal eÄŸitim tamamlandÄ±.\")\n",
    "print(\"Metrikler:\", summary)\n",
    "print(\"SonuÃ§ klasÃ¶rÃ¼:\", final_out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
